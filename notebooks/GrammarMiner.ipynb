{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Mining Input Grammars\n",
    "\n",
    "So far, the grammars we have seen have been mostly specified manually – that is, you (or the person knowing the input format) had to design and write a grammar in the first place.  While the grammars we have seen so far have been rather simple, creating a grammar for complex inoputs can involve quite some effort.  In this chapter, we therefore introduce techniques that automatically _mine_ grammars from programs – by executing the programs and observing how they process which parts of the input.  In conjunction with a grammar fuzzer, this allows us to (1) take a program, (2) extract its input grammar, and (3) fuzz it with high efficiency and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb).\n",
    "* The [chapter on configuration fuzzing](ConfigurationFuzzer.ipynb) introduces grammar mining for configuration options, as well as observing variables and values during execution.\n",
    "* The concept of parsing from [chapter on parsers](Parser.ipynb) is also useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the `process_inventory()`  method from the [chapter on parsers](Parser.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzingbook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import process_inventory, process_vehicle, process_car, process_van"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes inputs of the following form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVENTORY = \"\"\"\\\n",
    "1997,van,Ford,E350\n",
    "2000,car,Mercury,Cougar\n",
    "1999,car,Chevy,Venture\\\n",
    "\"\"\"\n",
    "print(process_inventory(INVENTORY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found from the [chapter on parsers](Parser.ipynb) that coarse grammars do not work well for fuzzing when the input format includes details expressed only in code. That is, even though we have the formal specification of CSV files ([RFC 4180](https://tools.ietf.org/html/rfc4180)), the inventory system includes further rules as to what is expected at each index of the CSV file. The solution of simply recombining existing inputs, while practical, is incomplete. In particular, it relies on a formal input specification being available in the first place. However, we have no assurance that the program obeys the input specification given.\n",
    "\n",
    "One of the ways out of this predicament is to interrogate the program under test as to what its input specification is. That is, if the program under test is written in a recursive descent style, with specific methods responsible for handling specific parts of the input, one can recover the parse tree, by observing the process of parsing. Further, one can recover a reasonable approximation of the grammar by abstraction from multiple input trees.\n",
    "\n",
    "The idea is as follows\n",
    "* The assumption (1) is that the program is written in such a fashion that specific methods are responsible for parsing specific fragments of the program. This includes almost all ad hoc parsers.\n",
    "* We hook into the Python execution and observe how the input fragments are produced and named in different methods.\n",
    "* Stitch the input fragments together in a tree structure to produce a parse tree.\n",
    "* Abstract common elements from multiple parse trees to produce the Context Free Grammar of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## A Simple Grammar Miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want to obtain the input grammar for the function `process_vehicle()`. We first collect the sample inputs for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES = INVENTORY.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen from the chapter on [configuration fuzzing](ConfigurationFuzzer.ipynb) that one can hook into the Python runtime to observe the arguments to a function and any local variables created. We have also seen that one can obtain the context of execution by inspecting the `frame` argument. Here is a simple tracer that can return the local variables and other contextual information in a traced function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceit(frame, event, arg):\n",
    "    method_name = frame.f_code.co_name\n",
    "    if method_name != \"process_vehicle\":\n",
    "        return\n",
    "    file_name = frame.f_code.co_filename\n",
    "    param_names = [frame.f_code.co_varnames[i] for i in range(frame.f_code.co_argcount)]\n",
    "    print(event, file_name, method_name, param_names, frame.f_locals)\n",
    "    return traceit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first obtain and save the current trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oldtrace = sys.gettrace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set our trace function as the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.settrace(traceit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code under this trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_vehicle(VEHICLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally reset the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.settrace(oldtrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interests of modularity, we expand the `traceit()` function to a full fledged class `Tracer` that acts as a *context manager*. A context manager in Python requires two methods `__enter__()` to enter the context and `__exit__()` to leave the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer:\n",
    "    def __enter__(self):\n",
    "        self.oldtrace = sys.gettrace()\n",
    "        sys.settrace(self.trace_event)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        sys.settrace(self.oldtrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic in the `traceit()` function is now moved to a method `trace_event()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def trace_event(self, frame, event, arg):\n",
    "        method_name = frame.f_code.co_name\n",
    "        if method_name != \"process_vehicle\":\n",
    "            return\n",
    "        file_name = frame.f_code.co_filename\n",
    "        param_names = [\n",
    "            frame.f_code.co_varnames[i]\n",
    "            for i in range(frame.f_code.co_argcount)\n",
    "        ]\n",
    "        print(event, file_name, method_name, param_names, frame.f_locals)\n",
    "        return self.trace_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That is, any function executed under it gets a tracing hook installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer() as tracer:\n",
    "    process_vehicle(VEHICLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trace_event()` relies on information from the `frame` variable which exposes Python internals. We define a `context` class that encapsulates the information that we need from the `frame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Context` class provides easy access to the information such as the current module, and parameter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    def __init__(self, frame, track_caller=True):\n",
    "        self.method = self._method(frame)\n",
    "        self.parameter_names = self._get_parameter_names(frame)\n",
    "        self.file_name = self._file_name(frame)\n",
    "        self.parent = Context(frame.f_back,\n",
    "                              False) if track_caller and frame.f_back else None\n",
    "\n",
    "    def _get_parameter_names(self, frame):\n",
    "        return [\n",
    "            frame.f_code.co_varnames[i]\n",
    "            for i in range(frame.f_code.co_argcount)\n",
    "        ]\n",
    "\n",
    "    def _file_name(self, frame):\n",
    "        return frame.f_code.co_filename\n",
    "\n",
    "    def _method(self, frame):\n",
    "        return frame.f_code.co_name\n",
    "\n",
    "    def extract_vars(self, frame):\n",
    "        return frame.f_locals\n",
    "\n",
    "    def parameters(self, all_vars):\n",
    "        return {\n",
    "            \"%s:%s\" % (self.method, k): v\n",
    "            for k, v in all_vars if k in self.parameter_names\n",
    "        }\n",
    "\n",
    "    def _t(self):\n",
    "        return (self.file_name, self.method, ','.join(self.parameter_names))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"%s:%s(%s)\" % self._t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the context can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceit(frame, event, arg):\n",
    "    cxt = Context(frame)\n",
    "    print((cxt.method, cxt.parameter_names, (cxt.parent.method, cxt.parent.parameter_names)))\n",
    "oldtrace = sys.gettrace()\n",
    "sys.settrace(traceit)\n",
    "process_vehicle(VEHICLES[0])\n",
    "sys.settrace(oldtrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace produced by executing any function can get overwhelmingly large. Hence, we need restrict our attention to specific modules. Further, we also restrict our attention exclusively to `str` variables since these variables are more likely to contain input fragments. (We will show how to deal with complex objects later.)\n",
    "\n",
    "We use the `context` to decide which modules to monitor, and which variables to trace.\n",
    "\n",
    "We store the current *input string* so that it can be used to determine if any particular string fragments came from the current input string. We add a `kwargs` for optional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def __init__(self, my_input, **kwargs):\n",
    "        self.my_input, self.trace = my_input, []\n",
    "        self.options(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the only optional argument is `files` which we use to indicate the specific source files we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def options(self, kwargs):\n",
    "        self.files = kwargs.get('files') or []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `files` is checked to determine if a particular event should be traced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def tracing_context(self, ctx, event, arg):\n",
    "        if not self.files:\n",
    "            return True\n",
    "        return any(ctx.file_name.endswith(f) for f in self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the context of events, we also want to restrict our attention to specific variables. For now, we want to focus only on strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def tracing_var(self, k, v):\n",
    "        return isinstance(v, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the `trace_event()` to call an `on_event()` function with the context information only on the specific events we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def on_event(self, event, arg, cxt, my_vars):\n",
    "        self.trace.append((event, arg, cxt, my_vars))\n",
    "\n",
    "    def trace_event(self, frame, event, arg):\n",
    "        cxt = Context(frame)\n",
    "        if not self.tracing_context(cxt, event, arg):\n",
    "            return self.trace_event\n",
    "\n",
    "        my_vars = [(k, v) for k, v in cxt.extract_vars(frame).items()\n",
    "                   if self.tracing_var(k, v)]\n",
    "        self.on_event(event, arg, cxt, my_vars)\n",
    "        return self.trace_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Tracer` class can now focus on specific kinds of events on specific files. Further, it provides a first level filter for variables that we find interesting. For example, we want to focus specifically on `string` variables that contain input fragments. Here is how our updated `Tracer` can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer(VEHICLES[0]) as tracer:\n",
    "    process_vehicle(VEHICLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution produced the following trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in tracer.trace:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are saving the input already in Tracer, it is redundant to specify it separately again as an argument. We modify our tracer to supply the saved argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(Tracer):\n",
    "    def __call__(self):\n",
    "        return self.my_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this change, the `Tracer` can be used as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer(VEHICLES[0]) as tracer:\n",
    "    process_vehicle(tracer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `settrace()` function hooks into the Python debugging facility. When it is in operation, no debugger can hook into the program. Hence, we limit the tracer to the simplest implementation possible as given above, and implement the core of grammar mining in later stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `Tracker` class that processes the trace from the `Tracer`.\n",
    "\n",
    "The tracker identifies string fragments that are part of the input string, and stores them in a dictionary `my_assignments`. It saves the trace, and the corresponding input for processing. Finally it calls `process()` to process the `trace` it was given. We additionally define a logging facility for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    def __init__(self, my_input, trace, **kwargs):\n",
    "        self.my_assignments = {}\n",
    "        self.trace = trace\n",
    "        self.my_input = my_input\n",
    "        self.options(kwargs)\n",
    "        self.process()\n",
    "\n",
    "    def options(self, kwargs):\n",
    "        self.log = kwargs.get('log') or False\n",
    "\n",
    "    def logger(self, var):\n",
    "        self.log and print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tracer simply records the variable values as they occur. We next need to check if the variables contain values from the **input string**. Common ways to do this is to rely on symbolic execution or at least dynamic tainting, which are powerful, but also complex. However, one can obtain a reasonable approximation by simply relying on substring search. That is, we consider any value produced that is a substring of the original input string to have come from the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems of using substring search is that short string sequences tend to be included in other string sequences even though they may not have come from the original string. That is, say the input fragment is `v`. It could have equally come from either `van` or `chevy`. We rely on being able to predict the exact place input where a given fragment occurred. Hence, we define a constant `FRAGMENT_LEN` such that we ignore strings up to that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAGMENT_LEN=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For longer strings, we rely on string inclusion to detect if the string came from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker(Tracker):\n",
    "    def include(self, var, value):\n",
    "        return len(value) > FRAGMENT_LEN and value in self.my_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tracker processes each event, and at each event, it updates the dictionary `my_assignments` with the current local variables that contain strings that are part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker(Tracker):\n",
    "    def track_event(self, event, arg, cxt, my_vars):\n",
    "        self.my_assignments.update({k: v for k, v in my_vars if self.include(k, v)})\n",
    "\n",
    "    def process(self):\n",
    "        for event, arg, cxt, my_vars in self.trace:\n",
    "            self.track_event(event, arg, cxt, my_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tracker, we can obtain the input fragments as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tracker = Tracker(tracer.my_input, tracer.trace)\n",
    "for k,v in tracker.my_assignments.items():\n",
    "    print(k, '=', repr(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining a Derivation Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input fragments from the `Tracker` only tell half the story. The fragments may be created at different stages of parsing. Hence, we need to assemble the fragments to a  derivation tree of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Grammars import START_SYMBOL, syntax_diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivation tree `Miner` is initialized with the input string, and the variable assignments, and it converts the assignments to the corresponding derivation tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner:\n",
    "    def __init__(self, my_input, my_assignments, **kwargs):\n",
    "        self.my_input = my_input\n",
    "        self.my_assignments = my_assignments\n",
    "        self.options(kwargs)\n",
    "        self.tree = self.get_derivation_tree()\n",
    "\n",
    "    def options(self, kwargs):\n",
    "        self.log = kwargs.get('log') or False\n",
    "\n",
    "    def logger(self, indent, var):\n",
    "        self.log and print('\\t' * indent, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is as follows:\n",
    "* We represent the derivation tree as a [straight line grammar](https://en.wikipedia.org/wiki/Straight-line_grammar) with each node represented by a key value pair. The key corresponds to the variable name, and the value corresponds to the representation of the value of the variable. The value representation may contain references to other nodes.\n",
    "* We start with a derivation tree with a single node -- the start symbol and the input string as its leaf.\n",
    "* For each pair _VAR_, _VALUE_ found in `my_assignments`:\n",
    "\n",
    "1. We search for occurrences of _VALUE_ in the grammar\n",
    "2. We replace them by <_VAR_>\n",
    "3. We add a new rule <_VAR_> $\\rightarrow$ <_VALUE_> to the grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a wrapper to generate a nonterminal from a variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nonterminal(var):\n",
    "    return \"<\" + var.lower() + \">\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main miner is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner(Miner):\n",
    "    def get_derivation_tree(self):\n",
    "        tree = {START_SYMBOL: (self.my_input, )}\n",
    "        my_assignments = self.my_assignments.copy()\n",
    "\n",
    "        while True:\n",
    "            new_rules = []\n",
    "            for var, value in my_assignments.items():\n",
    "                self.logger(0, \"%s = %s\" % (var, value))\n",
    "                for key, repl in tree.items():\n",
    "                    self.logger(1, \"%s : %s\" % (key, repl))\n",
    "                    if not any(value in t for t in repl):\n",
    "                        continue\n",
    "                    alt_key = to_nonterminal(var)\n",
    "                    new_arr = []\n",
    "                    for k, token in enumerate(repl):\n",
    "                        if not value in token:\n",
    "                            new_arr.append(token)\n",
    "                        else:\n",
    "                            arr = token.split(value)\n",
    "                            new_arr.extend(\n",
    "                                list(sum(zip(arr,\n",
    "                                             len(arr) * [alt_key]), ()))[:-1])\n",
    "                    tree[key] = tuple(i for i in new_arr if i)\n",
    "                    new_rules.append((var, alt_key, value))\n",
    "\n",
    "            if not new_rules:\n",
    "                break  # Nothing to expand anymore\n",
    "\n",
    "            for (var, alt_key, value) in new_rules:\n",
    "                tree[alt_key] = (value, )\n",
    "                self.logger(0, \"+%s = %s\" % (alt_key, value))\n",
    "\n",
    "                # Do not expand this again\n",
    "                del my_assignments[var]\n",
    "\n",
    "        return {key: values for key, values in tree.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Miner` is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer(VEHICLES[0]) as tracer:\n",
    "    process_inventory(tracer())\n",
    "assignments = Tracker(tracer.my_input, tracer.trace).my_assignments\n",
    "dt = Miner(tracer.my_input, assignments, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the grammar representation to a parse tree is accomplished by the `to_tree()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner(Miner):\n",
    "    def to_tree(self, key=START_SYMBOL):\n",
    "        if key not in self.tree:\n",
    "            return (key, [])\n",
    "        children = [self.to_tree(c) for c in self.tree[key]]\n",
    "        return (key, children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Miner(tracer.my_input, assignments).to_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "for VEHICLE in VEHICLES:\n",
    "    print(VEHICLE)\n",
    "    with Tracer(VEHICLE) as tracer:\n",
    "        process_inventory(tracer())\n",
    "    assignments = Tracker(tracer.my_input, tracer.trace).my_assignments\n",
    "    trees.append((tracer.my_input, assignments))\n",
    "    for var, val in assignments.items():\n",
    "        print(var + \" = \" + repr(val))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarFuzzer import GrammarFuzzer, FasterGrammarFuzzer, display_tree, tree_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_dt = []\n",
    "for inputstr, assignments in trees:\n",
    "    print(inputstr)\n",
    "    dt = Miner(inputstr, assignments)\n",
    "    csv_dt.append(dt)\n",
    "    display_tree(dt.to_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Recovering Grammar from Derivation Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class `Infer` that can combine multiple derivation trees to produce the grammar. The initial grammar is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infer:\n",
    "    def __init__(self):\n",
    "        self.grammar = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add_tree()` method gets a combined list of non-terminals from current grammar, and the tree to be added to the grammar, and updates the definitions of each non-terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infer(Infer):\n",
    "    def add_tree(self, t):\n",
    "        merged_grammar = {}\n",
    "        for key in list(self.grammar.keys()) + list(t.tree.keys()):\n",
    "            alternates = set(self.grammar.get(key, []))\n",
    "            if key in t.tree:\n",
    "                alternates.add(''.join(t.tree[key]))\n",
    "            merged_grammar[key] = list(alternates)\n",
    "        self.grammar = merged_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Infer` is used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Infer()\n",
    "for dt in csv_dt:\n",
    "    i.add_tree(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_diagram(i.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given execution traces from various inputs, one can define `recover_grammar()` to obtain the complete grammar from the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_grammar(traces):\n",
    "    m = Infer()\n",
    "    for inputstr, trace in traces:\n",
    "        dt = Miner(inputstr, Tracker(inputstr, trace).my_assignments)\n",
    "        m.add_tree(dt)\n",
    "    return m.grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1. Recovering the Inventory Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in VEHICLES:\n",
    "    with Tracer(inputstr) as tracer:\n",
    "        process_vehicle(tracer())\n",
    "    traces.append((tracer.my_input, tracer.trace))\n",
    "inventory_grammar = recover_grammar(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2. Recovering URL Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm is robust enough to recover grammar from real world programs. For example, the `urlparse` function in the Python `urlib` module accepts the following sample URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = [\n",
    "    'http://user:pass@www.google.com:80/?q=path#ref',\n",
    "    'https://www.cispa.saarland:80/',\n",
    "    'http://www.fuzzingbook.org/#News',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urllib caches its intermediate results for faster access. Hence, we need to disable it using `clear_cache()` after every invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, clear_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sample URLs to recover grammar as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in URLS:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr, files=['urllib/parse.py']) as tracer:\n",
    "        urlparse(tracer())\n",
    "    traces.append((tracer.my_input, tracer.trace))\n",
    "url_grammar = recover_grammar(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recovered grammar describes the URL format reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "syntax_diagram(url_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our recovered grammar for fuzzing as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = GrammarFuzzer(inventory_grammar)\n",
    "for i in range(10):\n",
    "    print(f.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recovered grammar can be used for fuzzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = GrammarFuzzer(url_grammar)\n",
    "for i in range(10):\n",
    "    print(f.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with the Simple Miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with our simple grammar miner is the assumption that variables have unique names. Unfortunately, that may not hold true in all cases. For example, here is a URL with a slightly different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS_X = URLS + ['ftp://freebsd.org/releases/5.8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar generated from this set of samples is not as nice as what we got earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in URLS_X:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr, files=['urllib/parse.py']) as tracer:\n",
    "        urlparse(tracer())\n",
    "    traces.append((tracer.my_input, tracer.trace))\n",
    "grammar = recover_grammar(traces)\n",
    "syntax_diagram(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate why the `url` and `scheme` definitions have gone wrong, let us inspect the trace for the newly added URL and a previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "with Tracer(URLS_X[0], log=True) as tracer:\n",
    "    urlparse(tracer())\n",
    "for i, t in enumerate(tracer.trace):\n",
    "    if t[0] in {'call', 'line'} and 'parse.py' in str(t[2]) and t[3]:\n",
    "        print(i, t[2]._t()[1], t[3:])\n",
    "print()\n",
    "clear_cache()\n",
    "with Tracer(URLS_X[-1], log=True) as tracer:\n",
    "    urlparse(tracer())\n",
    "for i, t in enumerate(tracer.trace):\n",
    "    if t[0] in {'call', 'line'} and 'parse.py' in str(t[2]) and t[3]:\n",
    "        print(i, t[2]._t()[1], t[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the value of `url` changes in `urlparse` and `urlsplit`? In `URLS[0]`, the value of `url` at `Line:60` is `/` which is ignored as its length is less than `FRAGMENT_LEN`. For these, the value of `url` used is the value from `Line:59`. On the other hand for `URLS_X[-1]`, the corresponding value of `url` at `Line:87` is `/releases/5.8` which is sufficient to overcome the `FRAGMENT_LEN` restriction.\n",
    "\n",
    "Is there a way to relax the `FRAGMENT_LEN` restriction? One way to do it is to restrict the amount of data we look at. If we can limit the variables inspected for inclusion to only those variables that are in context, and secondly, only those variables that have been assigned before, it might succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Miner with Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to relax the `FRAGMENT_LEN`, we incorporate inspection of the variables in the current context. For that, we define a class `InputStack` which holds the unmodified record of activation of the method. Essentially, we start with the original input at the base of the stack, and for each new method call we push the parameters of that call into the stack as a new record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStack:\n",
    "    def __init__(self, i):\n",
    "        self.original = i\n",
    "        self.inputs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check if a particular variable be saved, we define `in_current_record()` which checks only the last activation record for inclusion (rather than the original input string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class InputStack(InputStack):\n",
    "    def in_current_record(self, val):\n",
    "        return any(val in var for var in self.inputs[-1].values()) if self.inputs else val in self.original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can relax the FRAGMENT_LEN restriction, we can not do away with it completely. We only look at variables that are at least 2 characters long. We define the method `ignored()` that returns true if either the variable is not a string, or the variable length is less than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStack(InputStack):\n",
    "    def ignored(self, val):\n",
    "        return not (isinstance(val, str) and len(val) > 1) # FRAGMENT_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the `include()` method that checks whether the variable needs to be ignored, and if it is not to be ignored, whether the variable value is present in the activation record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStack(InputStack):\n",
    "    def include(self, k, val):\n",
    "        if self.ignored(val):\n",
    "            return False\n",
    "        return self.in_current_record(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define `push()` that pushes relevant variables in the current context to the activation record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStack(InputStack):\n",
    "    def push(self, inputs):\n",
    "        my_inputs = {k: v for k, v in inputs.items() if self.include(k, v)}\n",
    "        self.inputs.append(my_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a method returns, we also need a corresponding `pop()` to unwind the activation record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStack(InputStack):\n",
    "    def pop(self):\n",
    "        return self.inputs.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a convenience method `height()` that returns the height of the current activation record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStack(InputStack):\n",
    "    def height(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also proxy the variable dictionary so that it will only update if it does not already contain a value. For that, we define a new class `Vars` which represents our variables. The base input is associated with the start symbol. We also save the activation record in `istack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vars:\n",
    "    def __init__(self, stack):\n",
    "        self.defs = {START_SYMBOL: stack.original}\n",
    "        self.istack = stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary needs two methods: `update()` that takes a set of key-value pairs to update itself, and `set_kv()` that updates a particular key-value pair if the value does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vars(Vars):\n",
    "    def set_kv(self, k, v):\n",
    "        if k not in self.defs:\n",
    "            self.defs[k] = v\n",
    "\n",
    "    def update(self, v):\n",
    "        for k,v in v.items():\n",
    "            self.set_kv(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `InputStack` and `Vars` defined, we can now define the `StackTracker`. The `StackTracker` only saves variables if the value is present in the current activation record. To fine-tune the process, we define three optional parameters:\n",
    "* `track_params` -- if false, do not save any of the parameters in the variable dictionary\n",
    "* `track_vars` -- if false, only consider `call` and `return` events, ignoring `line`\n",
    "* `track_return` -- if true, add a virtual variable to the Vars representing the return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackTracker(Tracker):\n",
    "    def __init__(self, my_input, trace, **kwargs):\n",
    "        self.istack = InputStack(my_input)\n",
    "        self.my_assignments = Vars(self.istack)\n",
    "        self.trace = trace\n",
    "        self.options(kwargs)\n",
    "        self.process()\n",
    "\n",
    "    def options(self, kwargs):\n",
    "        self.track_params = kwargs.get('track_params') or True\n",
    "        self.track_vars = kwargs.get('track_vars') or True\n",
    "        self.track_return = kwargs.get('track_return') or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a wrapper for checking whether a variable is present in the activation record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackTracker(StackTracker):\n",
    "    def include(self, var, value):\n",
    "        return self.istack.include(var, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define methods `on_call`, `on_line` and `on_return` that is responsible for processing of corresponding events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackTracker(StackTracker):\n",
    "    def on_call(self, arg, cxt, my_vars):\n",
    "        my_parameters = {\n",
    "            k: v\n",
    "            for k, v in cxt.parameters(my_vars).items()\n",
    "            if not self.istack.ignored(v)\n",
    "        }\n",
    "        self.istack.push(my_parameters)\n",
    "        if self.track_params:\n",
    "            self.my_assignments.update(my_parameters)\n",
    "\n",
    "    def on_line(self, arg, cxt, my_vars):\n",
    "        if self.track_vars:\n",
    "            qvars = {\"%s:%s\" % (cxt.method, k): v for k, v in my_vars}\n",
    "            my_vars = {\n",
    "                var: value\n",
    "                for var, value in qvars.items() if self.include(var, value)\n",
    "            }\n",
    "            if not self.track_params:\n",
    "                my_vars = {\n",
    "                    var: value\n",
    "                    for var, value in my_vars.items() if var not in param_names\n",
    "                }\n",
    "            self.my_assignments.update(my_vars)\n",
    "\n",
    "    def on_return(self, arg, cxt, my_vars):\n",
    "        self.istack.pop()\n",
    "        self.on_line(arg, cxt, my_vars)\n",
    "        if self.track_return:\n",
    "            var = '(<-%s)' % cxt.method\n",
    "            if self.include(var, arg):\n",
    "                self.my_assignments.update({var: arg})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are called from `track_event()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackTracker(StackTracker):\n",
    "    def track_event(self, event, arg, cxt, my_vars):\n",
    "        if event == 'call':\n",
    "            return self.on_call(arg, cxt, my_vars)\n",
    "\n",
    "        if event == 'return':\n",
    "            return self.on_return(arg, cxt, my_vars)\n",
    "\n",
    "        if event == 'exception':\n",
    "            return\n",
    "\n",
    "        self.on_line(arg, cxt, my_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `StackTracker` as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url_traces = []\n",
    "for inputstr in VEHICLES:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr) as tracer:\n",
    "        urlparse(tracer())\n",
    "    sm = StackTracker(tracer.my_input, tracer.trace)\n",
    "    url_traces.append((tracer.my_input, sm))\n",
    "    for k,v in sm.my_assignments.defs.items():\n",
    "        print(k, v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining a Derivation Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier, while mining the derivation tree, we only look at variable assignments that happened *before* the current variable assignment took place. Note that we still assume that a particular variable is only assigned once. The algorithm is as follows\n",
    "\n",
    "For each (VAR, VALUE) found:\n",
    "* We search for occurrences of VALUE in the grammar\n",
    "* We replace them by VAR\n",
    "* We add a new rule VAR -> VALUE to the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Miner(Miner):\n",
    "    def get_derivation_tree(self):\n",
    "        my_assignments = self.my_assignments.copy()\n",
    "        tree = {}\n",
    "        for var, value in my_assignments.items():\n",
    "            nt_var = var if var == START_SYMBOL else to_nonterminal(var)\n",
    "            self.logger(0, \"%s = %s\" % (nt_var, value))\n",
    "            if tree:\n",
    "                append = False\n",
    "                for key, repl in tree.items():\n",
    "                    self.logger(1, \"%s : %s\" % (key, repl))\n",
    "                    if not any(value in t for t in repl):\n",
    "                        continue\n",
    "                    new_arr = []\n",
    "                    for k, token in enumerate(repl):\n",
    "                        if not value in token:\n",
    "                            new_arr.append(token)\n",
    "                        else:\n",
    "                            append = True\n",
    "                            arr = token.split(value)\n",
    "                            new_arr.extend(\n",
    "                                list(sum(zip(arr,\n",
    "                                             len(arr) * [nt_var]), ()))[:-1])\n",
    "                    tree[key] = tuple(i for i in new_arr if i)\n",
    "                if append:\n",
    "                    self.logger(0, \"+%s = %s\" % (nt_var, value))\n",
    "                    tree[nt_var] = set([value])\n",
    "            else:\n",
    "                tree[nt_var] = (value, )\n",
    "        return  {key: values for key, values in tree.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_grammar(traces):\n",
    "    m = Infer()\n",
    "    for inputstr, trace in traces:\n",
    "        st = StackTracker(inputstr, trace)\n",
    "        dt = Miner(inputstr, st.my_assignments.defs)\n",
    "        m.add_tree(dt)\n",
    "    return m.grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Recovering Inventory Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in VEHICLES:\n",
    "    with Tracer(inputstr) as tracer:\n",
    "        process_vehicle(tracer())\n",
    "    traces.append((tracer.my_input, tracer.trace))\n",
    "grammar = recover_grammar(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_diagram(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Recovering URL Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we obtain the derivation tree of the URL 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### URL 1 derivation tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "with Tracer(URLS_X[0], files=['urllib/parse.py']) as tracer:\n",
    "    urlparse(tracer())\n",
    "sm = StackTracker(tracer.my_input, tracer.trace)\n",
    "dt = Miner(tracer.my_input, sm.my_assignments.defs)\n",
    "display_tree(dt.to_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain the derivation tree of URL 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### URL 4 derivation tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clear_cache()\n",
    "with Tracer(URLS_X[-1], files=['urllib/parse.py']) as tracer:\n",
    "    urlparse(tracer())\n",
    "sm = StackTracker(tracer.my_input, tracer.trace)\n",
    "dt = Miner(tracer.my_input, sm.my_assignments.defs)\n",
    "display_tree(dt.to_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivation trees seem to belong to the same grammar. Hence, we obtain the grammar for the complete set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for inputstr in URLS_X:\n",
    "    clear_cache()\n",
    "    with Tracer(inputstr, files=['urllib/parse.py']) as tracer:\n",
    "        urlparse(tracer())\n",
    "    traces.append((tracer.my_input, tracer.trace))\n",
    "grammar = recover_grammar(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_diagram(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recovered grammar seems to have avoided the pitfall we discussed previously in the simple miner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with the stack miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned previously, our algorithm for mining grammars does not yet account for variable reassignments. The current `StackMiner` has problems when variables are reassigned which can happen due to loops or recursive calls. An example is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracer(INVENTORY) as tracer:\n",
    "    process_inventory(tracer())\n",
    "sm = StackTracker(tracer.my_input, tracer.trace)\n",
    "for k,v in sm.my_assignments.defs.items():\n",
    "    print(k,' = ',v)\n",
    "dt = Miner(tracer.my_input, sm.my_assignments.defs)\n",
    "display_tree(dt.to_tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the derivation tree obtained is not quite what we expected. We consider how to resolve this problem next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tainted Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InformationFlow import tstr, ctstr, rewrite_in, redefine_with_in, Instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_METHOD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_method():\n",
    "    return CURRENT_METHOD\n",
    "\n",
    "\n",
    "def set_current_method(method, method_stack):\n",
    "    global CURRENT_METHOD\n",
    "    CURRENT_METHOD = (method, len(method_stack), method_stack[-1])\n",
    "    return CURRENT_METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xtstr(ctstr):\n",
    "    def add_instr(self, op, c_a, c_b):\n",
    "        ct = None\n",
    "        if len(c_a) == 1 and isinstance(c_a, xtstr):\n",
    "            ct = c_a.taint[0]\n",
    "        elif len(c_b) == 1 and isinstance(c_b, xtstr):\n",
    "            ct = c_b.taint[0]\n",
    "        self.comparisons.append((ct, Instr(op, c_a, c_b), get_current_method()))\n",
    "\n",
    "    def create(self, res, taint):\n",
    "        o = xtstr(res, taint, self)\n",
    "        o.comparisons = self.comparisons\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we expand any object to a list of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(key, val):\n",
    "    # Should we limit flatened objects to repr ~ tstr here or during call?\n",
    "    tv = type(val)\n",
    "    if isinstance(val, (int, float, complex, str, bytes, bytearray)):\n",
    "        return [(key, val)]\n",
    "    elif isinstance(val, (set, frozenset, list, tuple, range)):\n",
    "        values = [e for i, elt in enumerate(val) for e in flatten(i, elt)]\n",
    "        return [(\"%s.%d\" % (key, i), v) for i, v in values]\n",
    "    elif isinstance(val, dict):\n",
    "        values = [e for k, elt in val.items() for e in flatten(k, elt)]\n",
    "        return [(\"%s.%s\" % (key, k), v) for k, v in values]\n",
    "    elif isinstance(val, tstr):\n",
    "        return [(key, val)]\n",
    "    elif hasattr(val,'__dict__'):\n",
    "        values = [e for k, elt in val.__dict__.items() for e in flatten(k, elt)]\n",
    "        return [(\"%s.%s\" % (key, k), v) for k, v in values]\n",
    "    else:\n",
    "        return [(key, repr(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple miner, we do not need the input stack. All that we need is the ability to identify reassignments in variables. However, it makes life a little simpler if we can annotate variables with the stack depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedStack(InputStack):\n",
    "    # height has ignored include push pop\n",
    "    def __init__(self, i):  # same as original\n",
    "        self.original = i\n",
    "        self.inputs = []\n",
    "\n",
    "    # has is used only with include\n",
    "    def has(self, val):\n",
    "        assert False\n",
    "\n",
    "    def ignored(self, val):\n",
    "        return not (isinstance(repr(val), tstr))\n",
    "\n",
    "    # used from push (and Tracker)\n",
    "    def include(self, k, val):\n",
    "        if self.ignored(val):\n",
    "            return False\n",
    "        return val.taint_in(self.original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to account for custom data structures other than containers is to rely on its `repr()`. That is, both `str()` and `repr()` relies on string methods that we have overridden in the tainted string. Hence if any of the string fragments are tainted, their return will also tainted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedVars(Vars):\n",
    "    def __init__(self, stack):\n",
    "        self.accessed_seq_var = {}\n",
    "        self.taint_register = {}\n",
    "        self.method_id = None\n",
    "        super().__init__(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedVars(TaintedVars):\n",
    "    def update(self, values):\n",
    "        vals = [(k1, v1) for k, v in values.items() for k1, v1 in flatten(k, v)]\n",
    "        for k, v in vals:\n",
    "            self.set_kv(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedVars(TaintedVars):\n",
    "    def set_current_method(self, method):\n",
    "        self.method_id = method\n",
    "\n",
    "    def var_init(self, var):\n",
    "        if var not in self.accessed_seq_var:\n",
    "            self.accessed_seq_var[var] = 0\n",
    "\n",
    "    def var_assign(self, var):\n",
    "        self.accessed_seq_var[var] += 1\n",
    "\n",
    "    def var_name(self, var):\n",
    "        var_seq = self.accessed_seq_var[var]\n",
    "        # TODO: figure out how to deal with stack/vars stack height\n",
    "        method, stack, method_seq = self.method_id\n",
    "        return \"%s[stack:%d mseq:%d vseq:%d]\" % (var, stack, method_seq, var_seq)\n",
    "\n",
    "    def set_kv(self, var, val):\n",
    "        self.var_init(var)\n",
    "        sa_var = self.var_name(var)\n",
    "        if sa_var not in self.defs:\n",
    "            self.defs[sa_var] = val\n",
    "            self.taint_register[str(\n",
    "                val.taint)] = self.taint_register.get(str(val.taint)) or set()\n",
    "            self.taint_register[str(val.taint)].add(sa_var)\n",
    "        else:  # possible reassignment\n",
    "            tainted_vars = self.taint_register.get(str(val.taint))\n",
    "            if tainted_vars is None or sa_var not in tainted_vars:  # a change in taint\n",
    "                self.var_assign(var)\n",
    "                sa_var = self.var_name(var)\n",
    "                self.defs[sa_var] = val\n",
    "                self.taint_register[str(val.taint)] = self.taint_register.get(\n",
    "                    str(val.taint)) or set()\n",
    "                self.taint_register[str(val.taint)].add(sa_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedTracer(Tracer):\n",
    "    def __init__(self, my_input, restrict={}):\n",
    "        self.my_input = xtstr(my_input, parent=None).with_comparisons([])\n",
    "        self.trace = []\n",
    "        self.restrict = restrict\n",
    "\n",
    "        self.method_num = 0\n",
    "        self.method_num_stack = [self.method_num]\n",
    "\n",
    "    def tracing_var(self, k, v):\n",
    "        return isinstance(repr(v), tstr)\n",
    "\n",
    "    def tracing_context(self, cxt, event, arg):\n",
    "        if self.restrict.get('files'):\n",
    "            return any(\n",
    "                cxt.file_name.endswith(f) for f in self.restrict['files'])\n",
    "        if self.restrict.get('methods'):\n",
    "            return cxt.method in self.restrict['methods']\n",
    "        return True\n",
    "\n",
    "    def on_event(self, event, arg, cxt, my_vars):\n",
    "        super().on_event(event, arg, cxt, my_vars)\n",
    "        if event == 'call':\n",
    "            self.method_num += 1\n",
    "            self.method_num_stack.append(self.method_num)\n",
    "        elif event == 'return':\n",
    "            self.method_num_stack.pop()\n",
    "        set_current_method(cxt.method, self.method_num_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedTracker(StackTracker):\n",
    "    def __init__(self, my_input, trace, **kwargs):\n",
    "        self.istack = TaintedStack(my_input)\n",
    "        self.my_input = my_input\n",
    "        self.my_assignments = TaintedVars(self.istack)\n",
    "        self.trace = trace\n",
    "        self.mstack = ['']\n",
    "        self.method_register = {}\n",
    "\n",
    "\n",
    "        self.options(kwargs)\n",
    "        self.process()\n",
    "\n",
    "    def call_enter(self, method):\n",
    "        key = (method, len(self.mstack))\n",
    "        if key not in self.method_register:\n",
    "            self.method_register[key] = 0\n",
    "        else:\n",
    "            self.method_register[key] += 1\n",
    "        method_id = (*key, self.method_register[key])\n",
    "        print(\"->\", \"\\t\" * len(self.mstack), method_id)\n",
    "        self.mstack.append(method_id)\n",
    "        self.my_assignments.set_current_method(self.mstack[-1])\n",
    "\n",
    "    def call_exit(self, method):\n",
    "        m = self.mstack.pop()\n",
    "        print(\"<-\", \"\\t\" * len(self.mstack), m)\n",
    "        self.my_assignments.set_current_method(self.mstack[-1])\n",
    "\n",
    "    def on_call(self, arg, cxt, my_vars):\n",
    "        self.call_enter(cxt.method)\n",
    "        super().on_call(arg, cxt, my_vars)\n",
    "\n",
    "    def on_return(self, arg, cxt, my_vars):\n",
    "        super().on_return(arg, cxt, my_vars)\n",
    "        self.call_exit(cxt.method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_grammar(traces):\n",
    "    m = Infer()\n",
    "    for inputstr, trace in traces:\n",
    "        st = TaintedTracker(inputstr, trace)\n",
    "        dt = Miner(inputstr, st.my_assignments.defs)\n",
    "        m.add_tree(dt)\n",
    "    return m.grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "restrict = {'methods':['process_inventory', 'process_vehicle', 'process_car', 'process_van']}\n",
    "with TaintedTracer(INVENTORY, restrict) as tracer:\n",
    "    process_inventory(tracer())\n",
    "sm = TaintedTracker(tracer.my_input, tracer.trace)\n",
    "#grammar = recover_grammar(traces)\n",
    "for k, v in sm.my_assignments.defs.items():\n",
    "    print(\"%s = <%s> \\t %s\" % (k,v, len(v.taint)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tainted Miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedMiner(Miner):\n",
    "    def get_derivation_tree(self):\n",
    "        my_assignments = self.my_assignments.copy()\n",
    "        root = (START_SYMBOL[1:-1], my_assignments[START_SYMBOL], [])\n",
    "        del my_assignments[START_SYMBOL]\n",
    "        for k, v in my_assignments.items():\n",
    "            self.insert_into_tree(root, (k, v, []))\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedMiner(TaintedMiner):\n",
    "    def insert_into_tree(self, root, elt):\n",
    "        # for each children of root, see if we are a\n",
    "        # subset of taints. If none, add ourself as a first level childA\n",
    "\n",
    "        ek, ev, eitems = elt\n",
    "        first_level = True\n",
    "        # first, look through the children to figure out if any one is a super set.\n",
    "        for child in root[2]:\n",
    "            if ev.taint_in(child[1]):\n",
    "                assert first_level\n",
    "                first_level = False\n",
    "                self.insert_into_tree(child, elt)\n",
    "\n",
    "        if not first_level:\n",
    "            return root\n",
    "\n",
    "        # then look through the children to figure out *how many* of them are subsets.\n",
    "        children = list(root[2])\n",
    "        new_children = []\n",
    "        while children:\n",
    "            child, *children = children\n",
    "            if child[1].taint_in(ev):\n",
    "                first_level = False\n",
    "                self.insert_into_tree(elt, child)\n",
    "            else:\n",
    "                new_children.append(child)\n",
    "\n",
    "        if not first_level:\n",
    "            new_children.append(elt)\n",
    "            root[2][:] = new_children\n",
    "            return\n",
    "\n",
    "        # neither superset or subset. Time to just append.\n",
    "        # check for overlap.\n",
    "        children = root[2]\n",
    "        for c in children:\n",
    "            for t in c[1].taint:\n",
    "                if t in ev.taint:\n",
    "                    assert False\n",
    "        root[2].append((ek, ev, eitems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that all children are in the right positions. In particular, assert that there are no overlaps. (If there are overlaps, we might have to choose between one of the overlapped elements based on the height of the tree of that element. -- Remember that each element is inserted as a child to *all* matching elements, and not just the first matching one. Hence, we can afford to choose between the trees and not worry about transferring elements across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(tree):\n",
    "    def simplify(var):\n",
    "        return re.sub(r'\\[.+\\]', '', var)\n",
    "    taint_start, taint_stop = tree[1].taint[0], tree[1].taint[-1]\n",
    "\n",
    "    new_children = []\n",
    "    prev_child = None\n",
    "    for child in sorted(tree[2], key=lambda c: c[1].taint[0]):\n",
    "        if prev_child:\n",
    "            assert prev_child[1].taint[0] < child[1].taint[0]\n",
    "        prev_child = child\n",
    "        child_t_start, child_t_stop = child[1].taint[0], child[1].taint[-1]\n",
    "        if child_t_start > taint_start:\n",
    "            elt = ''.join(tree[1].x(slice(taint_start, child_t_start)))\n",
    "            new_children.append((\"<%s>\" % repr(elt), [(repr(elt), [])]))\n",
    "        new_children.append(to_tree(child))\n",
    "        taint_start = child_t_stop + 1\n",
    "    if taint_start < taint_stop+1:\n",
    "        elt = ''.join(tree[1].x(slice(taint_start, taint_stop + 1)))\n",
    "        new_children.append((elt, []))\n",
    "    return (\"<%s>\" % simplify(tree[0]), new_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "restrict = {'methods':['process_inventory', 'process_vehicle', 'process_car', 'process_van']}\n",
    "with TaintedTracer(INVENTORY, restrict) as tracer:\n",
    "    process_inventory(tracer())\n",
    "sm = TaintedTracker(tracer.my_input, tracer.trace, track_return=True)\n",
    "dt = TaintedMiner(tracer.my_input, sm.my_assignments.defs)\n",
    "display_tree(to_tree(dt.tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_num(s,i):\n",
    "    n = ''\n",
    "    while s[i:] and s[i] in list(string.digits):\n",
    "        n += s[i]\n",
    "        i = i +1\n",
    "    return i,n\n",
    "\n",
    "def parse_paren(s, i):\n",
    "    assert s[i] == '('\n",
    "    i, v = parse_expr(s, i+1)\n",
    "    if s[i:] == '':\n",
    "        raise Exception(s, i)\n",
    "    assert s[i] == ')'\n",
    "    return i+1, v\n",
    "\n",
    "\n",
    "def parse_expr(s, i = 0):\n",
    "    expr = []\n",
    "    while s[i:]:\n",
    "        c = s[i]\n",
    "        if c in list(string.digits):\n",
    "            i,num = parse_num(s,i)\n",
    "            expr.append(num)\n",
    "        elif c in ['+', '-', '*', '/']:\n",
    "            expr.append(c)\n",
    "            i = i + 1\n",
    "        elif c == '(':\n",
    "            i, cexpr = parse_paren(s, i)\n",
    "            expr.append(cexpr)\n",
    "        elif c == ')':\n",
    "            return i, expr\n",
    "        else:\n",
    "            raise Exception(s,i)\n",
    "    return i, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astunparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in [parse_num, parse_paren, parse_expr]:\n",
    "    print(astunparse.unparse(rewrite_in(parse_num)))\n",
    "    redefine_with_in(parse_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPRS = [\n",
    "    '12+23',\n",
    "    '(25+1)+100+(33+2)+1',\n",
    "    '(25-1/(2+3))*100/3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parse_expr(xtstr(EXPRS[1]).with_comparisons([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(tree):\n",
    "    key, val, children = tree\n",
    "    taint_start, taint_stop = val.taint[0], val.taint[-1]\n",
    "\n",
    "    new_children = []\n",
    "    for child in sorted(children, key=lambda c: c[1].taint[0]):\n",
    "        ckey, cval, cchildren = child\n",
    "        child_t_start, child_t_stop = cval.taint[0], cval.taint[-1]\n",
    "        # if child taint starts after the root taint, then we have a child\n",
    "        # missing with index from taint_start to child_taint_start(non-incl)\n",
    "        if child_t_start > taint_start:\n",
    "            elt = ''.join(val.x(slice(taint_start, child_t_start)))\n",
    "            new_children.append((\"<%s>\" % repr(elt), [(repr(elt), [])]))\n",
    "        # and we append child as before.\n",
    "        new_children.append(to_tree(child))\n",
    "        # the new taint_start begins next to the taint stop.\n",
    "        taint_start = child_t_stop + 1\n",
    "    if taint_start < taint_stop + 1:\n",
    "        elt = ''.join(val.x(slice(taint_start, taint_stop + 1)))\n",
    "        new_children.append((elt, []))\n",
    "    return (\"<%s>\" % key, new_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(EXPRS[1])\n",
    "restrict = {'methods':['parse_expr', 'parse_paren', 'parse_num']}\n",
    "with TaintedTracer(EXPRS[1], restrict) as tracer:\n",
    "    parse_expr(tracer())\n",
    "sm = TaintedTracker(tracer.my_input, tracer.trace)\n",
    "dt = TaintedMiner(tracer.my_input, sm.my_assignments.defs)\n",
    "display_tree(to_tree(dt.tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tainted Miner (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse_trees(trace, inputstr):\n",
    "    # name,stack,children,idxs\n",
    "    def new_node(s, stack, mid=None, indexes=None):\n",
    "        n = {\n",
    "            'sym': '<%s>' % s,\n",
    "            'id': mid,\n",
    "            'stack': stack,\n",
    "            'indexes':  [],#[inputstr[i] for i in indexes] if indexes is not None else [],\n",
    "            'children': []\n",
    "        }\n",
    "        for i in (indexes or []):\n",
    "            n_addindex(n, i)\n",
    "        return n\n",
    "\n",
    "    root = new_node('start', stack=0, mid=0)\n",
    "    method_stack = [root]\n",
    "\n",
    "    def n_addchild(node, n): node['children'].append(n)\n",
    "    def n_mid(node):      return node['id']\n",
    "    def n_sym(node):      return node['sym']\n",
    "    def n_addindex(node, i):\n",
    "        c = inputstr[i]\n",
    "        #node['indexes'].append(c)\n",
    "        if node['children']:\n",
    "            last_child = node['children'][-1]\n",
    "            if last_child.get('id') is None: # last child an index. Hence, append\n",
    "                last_child['indexes'].append(c)\n",
    "                last_child['sym'] = ''.join(last_child['indexes'])\n",
    "            else: # last child a method. Hence, start a new node\n",
    "                n_addchild(node, {'sym': c, 'indexes': [c]})\n",
    "        else: #no last_child. Start a new node\n",
    "            n_addchild(node, {'sym': c, 'indexes': [c]})\n",
    "\n",
    "    print(inputstr)\n",
    "    prev_idx = None\n",
    "    prev_node = None\n",
    "    last_cmp_only = []\n",
    "    for idx, instr, (method_name, stack_len, mid) in trace:\n",
    "        if idx is None: continue\n",
    "        if idx == prev_idx:\n",
    "            prev_node = (idx, method_name, stack_len, mid)\n",
    "        else:\n",
    "            if prev_node is not None:\n",
    "                last_cmp_only.append(prev_node)\n",
    "            prev_node = (idx, method_name, stack_len, mid)\n",
    "            prev_idx = idx\n",
    "    if prev_node is not None:\n",
    "        last_cmp_only.append(prev_node)\n",
    "    print(len(last_cmp_only))\n",
    "    for x in last_cmp_only:\n",
    "        print(repr(x))\n",
    "        idx, method_name, stack_len, mid = x\n",
    "        print(\"%2d\" % idx, \" \", inputstr[idx], '  |' * stack_len, method_name,\n",
    "              mid, \"(%d)\" % stack_len)\n",
    "    print()\n",
    "    for idx, method_name, stack_len, mid in last_cmp_only:#[:4]:\n",
    "        last_idx = len(method_stack) -1\n",
    "        print(inputstr)\n",
    "        print(\"%2d\" % idx, \" chr:%s\" % inputstr[idx], '  |' * stack_len, \"\\t\",\n",
    "              method_name, mid, \"(%d)\" % stack_len)\n",
    "        if stack_len > last_idx:\n",
    "            print('###### append parent')\n",
    "            # first, generate a chain of the given stack length to the parent.\n",
    "            for i in range(len(method_stack), stack_len):\n",
    "                node = new_node('*',stack=i, mid=None)\n",
    "                n_addchild(method_stack[-1],node)\n",
    "                method_stack.append(node)\n",
    "\n",
    "            assert len(method_stack) == stack_len\n",
    "            # then for the given parent, add a child\n",
    "            node = new_node(method_name, stack=stack_len, mid=mid, indexes=[idx])\n",
    "            current = method_stack[-1]\n",
    "            n_addchild(current,node)\n",
    "            method_stack.append(node)\n",
    "        elif stack_len == last_idx:\n",
    "            current = method_stack[-1]\n",
    "            if n_mid(current) == mid:\n",
    "                print('###### insert char')\n",
    "                assert n_sym(current) == \"<%s>\" % method_name\n",
    "                current = method_stack[-1]\n",
    "                n_addindex(current, idx)\n",
    "            else:\n",
    "                print('###### start new peer node')\n",
    "                # a peer of this node. So get parent\n",
    "                method_stack.pop()\n",
    "                current = method_stack[-1]\n",
    "                node = new_node(method_name, stack=stack_len, mid=mid, indexes=[idx])\n",
    "                n_addchild(current,node)\n",
    "        elif stack_len < last_idx:\n",
    "            current = method_stack[-1]\n",
    "            print(\"-\", current)\n",
    "            while stack_len < last_idx:\n",
    "                print('<-')\n",
    "                method_stack.pop()\n",
    "                last_idx = len(method_stack)-1\n",
    "            assert stack_len == last_idx\n",
    "            current = method_stack[-1]\n",
    "            \n",
    "            if n_mid(current) is None:\n",
    "                current['id'] = mid\n",
    "                current['sym'] = \"<%s>\" % method_name\n",
    "            else:\n",
    "                assert n_mid(current) == mid, \"%s != %s\" %(n_mid(current), mid)\n",
    "                assert n_sym(current) == \"<%s>\" % method_name\n",
    "            current = method_stack[-1]\n",
    "            n_addindex(current, idx)\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_root = parse_trees(tracer.my_input.comparisons, str(tracer.my_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i in tracer.my_input.comparisons:\n",
    "#    print(repr(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(node, my_str):\n",
    "    sym = node['sym']\n",
    "    children = node.get('children')\n",
    "    if children:\n",
    "        return (sym, [to_tree(c, my_str) for c in children])\n",
    "    else:\n",
    "        return (sym, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tree(my_root, str(tracer.my_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tracer.my_input)\n",
    "tree = to_tree(my_root, str(tracer.my_input))\n",
    "display_tree(tree)\n",
    "assert tree_to_string(tree) == str(tracer.my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import PEGParser, VAR_GRAMMAR, VAR_TOKENS, canonical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = 'a=1'\n",
    "#mystring = 'avar=1.3;bvar=avar-3*(4+300)'\n",
    "C_VG = canonical(VAR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_key(grammar, key, text, at=0):\n",
    "    if key not in grammar:\n",
    "        if text[at:].startswith(key):\n",
    "            return at + len(key), (key, [])\n",
    "        else:\n",
    "            return at, None\n",
    "    for rule in grammar[key]:\n",
    "        to, res = unify_rule(grammar, rule, text, at)\n",
    "        if res:\n",
    "            return (to, (key, res))\n",
    "    return 0, None\n",
    "\n",
    "def unify_rule(grammar, rule, text, at):\n",
    "    results = []\n",
    "    for token in rule:\n",
    "        at, res = unify_key(grammar, token, text, at)\n",
    "        if res is None:\n",
    "            return at, None\n",
    "        results.append(res)\n",
    "    return at, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unify_key(C_VG, '<start>', mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaintedTracer(Tracer):\n",
    "    def __init__(self, my_input, restrict={}):\n",
    "        self.my_input = xtstr(my_input, parent=None).with_comparisons([])\n",
    "        self.trace = []\n",
    "        self.restrict = restrict\n",
    "\n",
    "        self.method_num = 0\n",
    "        start = (self.method_num, None, [], {})\n",
    "        self.method_num_stack = [start]\n",
    "        self.method_map = {self.method_num: start}\n",
    "\n",
    "    def tracing_var(self, k, v):\n",
    "        return isinstance(repr(v), tstr)\n",
    "\n",
    "    def tracing_context(self, cxt, event, arg):\n",
    "        if self.restrict.get('files'):\n",
    "            return any(\n",
    "                cxt.file_name.endswith(f) for f in self.restrict['files'])\n",
    "        if self.restrict.get('methods'):\n",
    "            return cxt.method in self.restrict['methods']\n",
    "        return True\n",
    "\n",
    "    def on_event(self, event, arg, cxt, my_vars):\n",
    "        # make it tree\n",
    "        super().on_event(event, arg, cxt, my_vars)\n",
    "        if event == 'call':\n",
    "            self.method_num += 1\n",
    "            n = (self.method_num, cxt.method, [], {})\n",
    "            self.method_map[self.method_num] = n\n",
    "            self.method_num_stack[-1][2].append(n)\n",
    "            self.method_num_stack.append(n)\n",
    "        elif event == 'return':\n",
    "            self.method_num_stack.pop()\n",
    "        set_current_method(cxt.method, self.method_num_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_parent_map(method_map):\n",
    "    parent_map = {}\n",
    "    for i in method_map:\n",
    "        mid, method, children, ann = method_map[i]\n",
    "        for cmid,*_ in children:\n",
    "            parent_map[cmid] = mid\n",
    "    return parent_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parse_trees(trace, inputstr, method_map):\n",
    "    def add_indexes(node, indexes):\n",
    "        for i in indexes:\n",
    "            n = new_node(i)\n",
    "            node['children'].append(n)\n",
    "    parent_map = to_parent_map(method_map)\n",
    "\n",
    "    # name,stack,children,idxs\n",
    "\n",
    "    def new_node(s, mid=None):\n",
    "        return {\n",
    "            'sym': s,\n",
    "            'id': mid,\n",
    "            'children': []\n",
    "        }\n",
    "\n",
    "    root = new_node(\"<%s>\" % 'start', mid=0)\n",
    "    method_stack = [root]\n",
    "    method_stack_map = {s['id'] for s in method_stack}\n",
    "    last_cmp_only = {}\n",
    "    for idx, instr, (method_name, stack_len, minfo) in trace:\n",
    "        if idx is None: continue  # TODO. investigate None idx in IF\n",
    "        last_cmp_only[idx] = (idx, method_name, stack_len, minfo[0])\n",
    "        \n",
    "    for x in last_cmp_only.values():\n",
    "        idx, method_name, stack_len, mid = x\n",
    "        print(\"%2d\" % idx, \" \", inputstr[idx], '  |' * stack_len, method_name, mid, \"(%d)\" % stack_len)\n",
    "    print()\n",
    "    \n",
    "    last_cmp_values = list(last_cmp_only.values())\n",
    "    for idx, m_name, stack_len, mid in last_cmp_values:\n",
    "        print(\"%2d\" % idx, repr(inputstr[idx]), '  |' * stack_len, m_name, mid, \"(%d)\" % stack_len)\n",
    "        current = method_stack[-1]\n",
    "        # look back until we have a parent that is in method_stack_map\n",
    "        parent_mid = mid\n",
    "        parents = [parent_mid]\n",
    "        # look for any parent that is in the stack so we can budd off\n",
    "        # from that point\n",
    "        while parent_mid not in method_stack_map:\n",
    "            parent_mid = parent_map[parent_mid]\n",
    "            parents.insert(0, parent_mid)\n",
    "        # at this point, we have found a common parent. Now, pop off the\n",
    "        # stack so that we reach the common parent.\n",
    "\n",
    "        # pop off the method_stack until we come to the parents[0]\n",
    "        while parents[0] != method_stack[-1]['id']:\n",
    "            v = method_stack.pop()\n",
    "            method_stack_map.remove(v['id'])\n",
    "\n",
    "        # now start appending until we reach mid\n",
    "        # note that we also append a node with mid\n",
    "        for elt in parents[1:]:\n",
    "            e_mid, e_method, e_children, e_ann = method_map[elt]\n",
    "            idxs = [] if e_mid != mid else [inputstr[idx]]\n",
    "            node = new_node(\"<%s>\" % e_method, mid=elt)\n",
    "            add_indexes(node, idxs)\n",
    "            method_stack[-1]['children'].append(node)\n",
    "            method_stack.append(node)\n",
    "            method_stack_map.add(node['id'])\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tree(node, my_str):\n",
    "    sym = node['sym']\n",
    "    children = node.get('children')\n",
    "    if children:\n",
    "        return (sym, [to_tree(c, my_str) for c in children])\n",
    "    else:\n",
    "        return (sym, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict = {'methods':['unify_key', 'unify_rule']}\n",
    "mystring = 'a=1.3;b=a-3'\n",
    "with TaintedTracer(mystring, restrict) as tracer:\n",
    "    unify_key(C_VG, '<start>', tracer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracer.method_map[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_root = parse_trees(tracer.my_input.comparisons, str(tracer.my_input), tracer.method_map)\n",
    "#print(my_root)\n",
    "tree = to_tree(my_root, str(tracer.my_input))\n",
    "#print(tree)\n",
    "display_tree(tree)\n",
    "assert tree_to_string(tree) == str(tracer.my_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* Given a set of inputs, we can learn an input grammar by examining variable values during execution.\n",
    "* The resulting grammars can be used right during fuzzing.\n",
    "* TODO: make the point that our initial implementation is about learning regular grammar not CFG because we do not know how to handle mutually recursive and looping procedures\n",
    "* TODO: Use process_vehicle as a pervading example.\n",
    "* TODO: Mention that control flow dependencies is not tracked in dynamic taints. But it is tracked in simple miner with string inclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _mutations_ on existing inputs to get more valid inputs](MutationFuzzer.ipynb)\n",
    "* [use _grammars_ (i.e., a specification of the input format) to get even more valid inputs](Grammars.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](Reducer.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "\\cite{Lin2008}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "_Close the chapter with a few exercises such that people have things to do.  To make the solutions hidden (to be revealed by the user), have them start with_\n",
    "\n",
    "```markdown\n",
    "**Solution.**\n",
    "```\n",
    "\n",
    "_Your solution can then extend up to the next title (i.e., any markdown cell starting with `#`)._\n",
    "\n",
    "_Running `make metadata` will automatically add metadata to the cells such that the cells will be hidden by default, and can be uncovered by the user.  The button will be introduced above the solution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2: _Title_\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Solution for the exercise_"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.2",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
