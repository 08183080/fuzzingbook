{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Parsing and Recombining Inputs\n",
    "\n",
    "In this chapter, we use grammars to parse and decompose inputs, allowing us to recombine them arbitrarily.\n",
    "\\todo{Work in progress.}\n",
    "* Specify the restrictions due to precedence\n",
    "* Specify the class of languages covered (what is the overlap with CFG)\n",
    "* Specify the restrictions on our implementation (parsing predicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import fuzzingbook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from Grammars import EXPR_GRAMMAR, START_SYMBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from GrammarFuzzer import display_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import re\n",
    "\n",
    "RE_NONTERMINAL = re.compile(r'(<[a-zA-Z_]*>)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser:\n",
    "    def __init__(self, grammar):\n",
    "        def split(rule):\n",
    "            return tuple(s for s in\n",
    "                         re.split(RE_NONTERMINAL, rule) if s)\n",
    "        self.grammar = {k: [split(l) for l in rules]\n",
    "                        for k, rules in grammar.items()}\n",
    "\n",
    "    def literal_match(self, part, text, cursor):\n",
    "        if text[cursor:].startswith(part):\n",
    "            return (cursor + len(part), (part, []))\n",
    "        else:\n",
    "            return (cursor, None)\n",
    "\n",
    "    # memoize repeated calls.\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def unify_key(self, key, text, cursor=0):\n",
    "        rules = self.grammar[key]\n",
    "        # make a generator for matching rules. We dont want a list because\n",
    "        # we want to be lazy and evaluate only until the first matching\n",
    "        rets = (self.unify_line(rule, text, cursor) for rule in rules)\n",
    "        # return the first non null (matching) rule's cursor and res\n",
    "        cursor, res = next(\n",
    "            (ret for ret in rets if ret[1] is not None), (cursor, None))\n",
    "        return (cursor, (key, res) if res is not None else None)\n",
    "\n",
    "    def unify_line(self, parts, text, cursor):\n",
    "        def is_symbol(v): return v[0] == '<'\n",
    "\n",
    "        results = []\n",
    "        for part in parts:\n",
    "            # get the matcher function\n",
    "            matcher = (self.unify_key\n",
    "                       if is_symbol(part) else self.literal_match)\n",
    "            # compute the cursor, and the result from it.\n",
    "            cursor, res = matcher(part, text, cursor)\n",
    "            if res is None:\n",
    "                return (cursor, None)\n",
    "            results.append(res)\n",
    "        return cursor, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text, grammar, start_symbol=START_SYMBOL):\n",
    "    def readall(fn): return ''.join([f for f in open(fn, 'r')]).strip()\n",
    "\n",
    "    result = PEGParser(grammar).unify_key(start_symbol, text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor, tree = parse(\"1 + 2 * 3\", EXPR_GRAMMAR)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table driven parsers\n",
    "\n",
    "Parsing Expression Grammars specifically oriented towards writing recognizers. Unfortunately, Parsing Expression Grammars are not suitable for grammar based fuzzing. \\todo{Verify, and explain how precedence in parsing is not translatable to generation}.\n",
    "\\todo{Explain LL(k), LR(k), and general Context-Free parsers such as Early and CYK parsers}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LL(1) parser\n",
    "\n",
    "LL(k) parsers are top-down parsers that rely on a lookahead of k tokens. We provide an implementation of an LL(1) parser.\n",
    "\n",
    "We first need a few tools to massage the grammar. The `split_tokens` splits a production rule to its constituent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens(rule):\n",
    "    return [i for i in re.split(RE_NONTERMINAL, rule) if i != ''] or ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LL(1) grammars are rather restrictive. Specifically, the grammar should not contain left recursion. Hence, we have to\n",
    "update our original grammar to remove left-recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {'<start>': ['<expr>'],\n",
    "           '<expr>': ['<term><expr_>'],\n",
    "           '<expr_>': ['+<expr>',\n",
    "                       '-<expr>',\n",
    "                       ''],\n",
    "           '<term>': ['<factor><term_>'],\n",
    "           '<term_>': ['*<term>',\n",
    "                       '/<term>',\n",
    "                       ''],\n",
    "           '<factor>': ['+<factor>',\n",
    "                        '-<factor>',\n",
    "                        '(<expr>)',\n",
    "                        '<int>'],\n",
    "           '<int>': ['<integer><integer_>'],\n",
    "           '<integer_>': ['',\n",
    "                          '.<integer>'],\n",
    "           '<integer>': ['<digit><I>'],\n",
    "           '<I>': ['<integer>',\n",
    "                   ''],\n",
    "           '<digit>': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to change the grammar so that the productions become tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [split_tokens(e) for e in grammar[k]] for k in grammar}\n",
    "new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the listing of production rules, and the set of terminals in the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(g): return [(k, e) for k, a in g.items() for e in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminals(g):\n",
    "    return set(token for k, expr in rules(g)\n",
    "               for token in expr if token not in g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and Follow sets\n",
    "\n",
    "\\todo{Define first and follow sets}\n",
    "We first define the fixpiont\n",
    "\\todo{Define what is a fixpoint of a function}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixpoint(f):\n",
    "    def helper(*args):\n",
    "        while True:\n",
    "            sargs = repr(args)\n",
    "            args_ = f(*args)\n",
    "            if repr(args_) == sargs:\n",
    "                return args\n",
    "            args = args_\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fixpoint\n",
    "def compute_ff(grammar, first, follow, epsilon):\n",
    "    for A, expression in rules(grammar):\n",
    "        nullable = True\n",
    "        for token in expression:\n",
    "            first[A] |= first[token]\n",
    "\n",
    "            # update until the first token that is not nullable\n",
    "            if token not in epsilon:\n",
    "                nullable = False\n",
    "                break\n",
    "\n",
    "        if nullable:\n",
    "            epsilon |= {A}\n",
    "            first[A] |= {''}\n",
    "\n",
    "        # https://www.cs.uaf.edu/~cs331/notes/FirstFollow.pdf\n",
    "        # essentially, we start from the end of the expression. Then:\n",
    "        # (3) if there is a production A -> aB, then every thing in\n",
    "        # FOLLOW(A) is in FOLLOW(B)\n",
    "        follow_B = follow[A]\n",
    "        for t in reversed(expression):\n",
    "            if not t:\n",
    "                continue\n",
    "            # update the follow for the current token. If this is the\n",
    "            # first iteration, then here is the assignment\n",
    "            if t in grammar:\n",
    "                follow[t] |= follow_B  # only bother with nt\n",
    "\n",
    "            # computing the last follow symbols for each token t. This\n",
    "            # will be used in the next iteration. If current token is\n",
    "            # nullable, then previous follows can be a legal follow for\n",
    "            # next. Else, only the first of current token is legal follow\n",
    "            # essentially\n",
    "\n",
    "            # (2) if there is a production A -> aBb then everything in FIRST(B)\n",
    "            # except for epsilon is added to FOLLOW(B)\n",
    "            follow_B = follow_B | (\n",
    "                first[t] - {''}) if t in epsilon else first[t]\n",
    "\n",
    "    return (grammar, first, follow, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(grammar):\n",
    "    # Initialize first and follow sets for non-terminals\n",
    "    first = {i: set() for i in grammar}\n",
    "    follow = {i: set() for i in grammar}\n",
    "\n",
    "    # If X is a terminal, then First(X) is just X\n",
    "    first.update((i, {i}) for i in terminals(grammar))\n",
    "    epsilon = {''}\n",
    "    return compute_ff(grammar, first, follow, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnullable(rule, epsilon):\n",
    "    return all(token in epsilon for token in rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfirst(rule, first, epsilon):\n",
    "    tokens = set()\n",
    "    for token in rule:\n",
    "        tokens |= first[token]\n",
    "        if token not in epsilon:\n",
    "            break  # not nullable\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(rulepair, first, follow, epsilon):\n",
    "    A, rule = rulepair\n",
    "    rf = rfirst(rule, first, epsilon)\n",
    "    if rnullable(rule, epsilon):\n",
    "        rf |= follow[A] - {''}\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table(grammar, my_rules):\n",
    "    _, first, follow, epsilon = process(grammar)\n",
    "\n",
    "    ptable = [(rule, predict(rule, first, follow, epsilon))\n",
    "              for rule in my_rules]\n",
    "\n",
    "    parse_tbl = {k: {} for k in grammar}\n",
    "\n",
    "    for (k, expr), pvals in ptable:\n",
    "        parse_tbl[k].update({v: (k, expr) for v in pvals})\n",
    "    return parse_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_helper(grammar, tbl, stack, inplst):\n",
    "    inp, *inplst = inplst\n",
    "    exprs = []\n",
    "    while stack:\n",
    "        val, *stack = stack\n",
    "        if isinstance(val, tuple):\n",
    "            exprs.append(val)\n",
    "            continue\n",
    "        if val not in grammar:  # terminal\n",
    "            if val == '':\n",
    "                exprs.append(val)\n",
    "                continue\n",
    "            if val != inp:\n",
    "                raise Exception(\"%s != %s\" % (val, inp))\n",
    "            exprs.append(val)\n",
    "            inp, *inplst = [*inplst, '']\n",
    "        else:\n",
    "            k, rhs = tbl[val][inp]\n",
    "            assert k == val\n",
    "            stack = rhs + [(val, len(rhs))] + stack\n",
    "    return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(grammar, inp):\n",
    "    my_rules = rules(grammar)\n",
    "    parse_tbl = parse_table(grammar, my_rules)\n",
    "    k, _ = my_rules[0]\n",
    "    stack = [k]\n",
    "    return parse_helper(grammar, parse_tbl, stack, list(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_to_tree(arr):\n",
    "    stack = []\n",
    "    while arr:\n",
    "        elt = arr.pop(0)\n",
    "        if not isinstance(elt, tuple):\n",
    "            stack.append((elt, []))\n",
    "        else:\n",
    "            # get the last n\n",
    "            sym, n = elt\n",
    "            elts = stack[-n:]\n",
    "            stack = stack[0:len(stack) - n]\n",
    "            stack.append((sym, elts))\n",
    "    assert len(stack) == 1\n",
    "    return stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = linear_to_tree(parse(new_grammar, '(1+2)'))\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earley parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_convert(rule): return [i.strip() for i in rule if i != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [epsilon_convert(split_tokens(e)) for e in EXPR_GRAMMAR[k]] for k in EXPR_GRAMMAR}\n",
    "new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fixpoint\n",
    "def nullable_(rules, e):\n",
    "    for A, expression in rules:\n",
    "        if all((token in e)  for token in expression): e |= {A}\n",
    "    return (rules, e)\n",
    "\n",
    "def nullable(grammar):\n",
    "    return nullable_(rules(grammar), set())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, name, expr, dot, origin, children=[]):\n",
    "        self.name, self.expr, self.dot, self.origin = name, expr, dot, origin\n",
    "        self.children = children[:]\n",
    "    def finished(self): return self.dot >= len(self.expr)\n",
    "    def shift(self):\n",
    "        return State(self.name, self.expr, self.dot+1, self.origin, self.children)\n",
    "    def symbol(self): return self.expr[self.dot]\n",
    "\n",
    "    def _t(self): return (self.name, self.expr, self.dot, self.origin.i, tuple(self.children))\n",
    "    def __hash__(self): return hash(self._t())\n",
    "    def __eq__(self, other): return  self._t() == other._t()\n",
    "\n",
    "class Column(object):\n",
    "    def __init__(self, i, token):\n",
    "        self.token, self.states, self._unique, self.i = token, [], {}, i\n",
    "\n",
    "    def add(self, state):\n",
    "        if state in self._unique: return self._unique[state]\n",
    "        self._unique[state] = state\n",
    "        self.states.append(state)\n",
    "        return self._unique[state]\n",
    "\n",
    "def predict(col, sym, grammar):\n",
    "    for alt in grammar[sym]:\n",
    "        col.add(State(sym, tuple(alt), 0, col))\n",
    "\n",
    "def scan(col, state, token):\n",
    "    if token == col.token:\n",
    "        col.add(state.shift())\n",
    "\n",
    "def complete(col, state, grammar):\n",
    "    for st in state.origin.states:\n",
    "        if st.finished(): continue\n",
    "        if state.name != st.symbol(): continue\n",
    "        col.add(st.shift()).children.append(state)\n",
    "\n",
    "# http://courses.washington.edu/ling571/ling571_fall_2010/slides/parsing_earley.pdf\n",
    "# https://github.com/tomerfiliba/tau/blob/master/earley3.py\n",
    "def parse(words, grammar, start):\n",
    "    # Aycock 2002 Practical Earley Parsing -- treatment of epsilon\n",
    "    epsilon = nullable(grammar)\n",
    "    alt = tuple(*grammar[start])\n",
    "    chart = [Column(i, tok) for i,tok in enumerate([None, *words])]\n",
    "    chart[0].add(State(start, alt, 0, chart[0], []))\n",
    "\n",
    "    for i, col in enumerate(chart):\n",
    "        for state in col.states:\n",
    "            if state.finished():\n",
    "                complete(col, state, grammar)\n",
    "            else:\n",
    "                sym = state.symbol()\n",
    "                if sym in grammar:\n",
    "                    predict(col, sym, grammar)\n",
    "                    if sym in epsilon:\n",
    "                        # note that precomputation of epsilon derivation can result in infinite\n",
    "                        # loops for certain grammars. Hence, we mark a nullable non-terminal\n",
    "                        # but do not expand it.\n",
    "                        col.add(state.shift()).children.append(State(sym + '*', tuple(), 0, col))\n",
    "                else:\n",
    "                    if i + 1 >= len(chart): continue\n",
    "                    scan(chart[i+1], state, sym)\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_expr(expr, children, grammar):\n",
    "    terms = iter([(i,[]) for i in expr if i not in grammar])\n",
    "    nts = iter([node_translator(i, grammar) for i in  children])\n",
    "    return [next(terms if i not in grammar else nts) for i in expr]\n",
    "\n",
    "def node_translator(state, grammar):\n",
    "    return (state.name, process_expr(state.expr, state.children, grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [epsilon_convert(split_tokens(e)) for e in EXPR_GRAMMAR[k]] for k in EXPR_GRAMMAR}\n",
    "table = parse(list('1+2+3'), new_grammar, '<start>')\n",
    "states = [st for st in table[-1].states if st.name == '<start>' and st.finished()]\n",
    "for state in states:\n",
    "    display_tree(node_translator(state, new_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguous grammars generates parse forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar= {\n",
    "        '<start>': ['<A>'],\n",
    "        '<A>': ['<A>+<A>', 'a'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [epsilon_convert(split_tokens(e)) for e in grammar[k]] for k in grammar}\n",
    "table = parse(list('a+a+a'), new_grammar, '<start>')\n",
    "states = [st for st in table[-1].states if st.name == '<start>' and st.finished()]\n",
    "for state in states:\n",
    "    display_tree(node_translator(state, new_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Lesson one_\n",
    "* _Lesson two_\n",
    "* _Lesson three_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _mutations_ on existing inputs to get more valid inputs](MutationFuzzer.ipynb)\n",
    "* [use _grammars_ (i.e., a specification of the input format) to get even more valid inputs](Grammars.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](Reducing.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Close the chapter with a few exercises such that people have things to do.  In Jupyter Notebook, use the `exercise2` nbextension to add solutions that can be interactively viewed or hidden:\n",
    "\n",
    "* Mark the _last_ cell of the exercise (this should be a _text_ cell) as well as _all_ cells of the solution.  (Use the `rubberband` nbextension and use Shift+Drag to mark multiple cells.)\n",
    "* Click on the `solution` button at the top.\n",
    "\n",
    "(Alternatively, just copy the exercise and solution cells below with their metadata.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "_Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "_Solution for the exercise_"
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
