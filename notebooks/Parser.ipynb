{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Parsing and Recombining Inputs\n",
    "\n",
    "In this chapter, we use grammars to parse and decompose inputs, allowing us to recombine them arbitrarily.\n",
    "\\todo{Work in progress.}\n",
    "* Specify the restrictions due to precedence\n",
    "* Specify the class of languages covered (what is the overlap with CFG)\n",
    "* Specify the restrictions on our implementation (parsing predicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import fuzzingbook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from Grammars import EXPR_GRAMMAR, START_SYMBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from GrammarFuzzer import display_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import re\n",
    "\n",
    "RE_NONTERMINAL = re.compile(r'(<[a-zA-Z_]*>)')\n",
    "\n",
    "def split(rule):\n",
    "    return [s for s in re.split(RE_NONTERMINAL, rule) if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = {k: [split(l) for l in rules]\n",
    "                        for k, rules in grammar.items()}\n",
    "    # memoize repeated calls.\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if key not in self.grammar:\n",
    "            if text[at:].startswith(key): return at + len(key), (key, [])\n",
    "            else: return at, None\n",
    "        rules = self.grammar[key]\n",
    "        for rule in rules:\n",
    "            l, res = self.unify_line(rule, text, at)\n",
    "            if res: return (l, (key, res))\n",
    "        return 0, None\n",
    "\n",
    "    def unify_line(self, rule, text, at):\n",
    "        results = []\n",
    "        for token in rule:\n",
    "            at, res = self.unify_key(token, text, at)\n",
    "            if res is None: return at, None\n",
    "            results.append(res)\n",
    "        return at, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text, grammar, start_symbol=START_SYMBOL):\n",
    "    peg = PEGParser(grammar)\n",
    "    return peg.unify_key(start_symbol, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor, tree = parse(\"1 + (2 * 3)\", EXPR_GRAMMAR)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor, tree = parse(\"1 * (2 + 3.45)\", EXPR_GRAMMAR)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table driven parsers\n",
    "\n",
    "Parsing Expression Grammars specifically oriented towards writing recognizers. Unfortunately, Parsing Expression Grammars are not suitable for grammar based fuzzing. \\todo{Verify, and explain how precedence in parsing is not translatable to generation}.\n",
    "\\todo{Explain LL(k), LR(k), and general Context-Free parsers such as Early and CYK parsers}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LL(1) parser\n",
    "\n",
    "LL(k) parsers are top-down parsers that rely on a lookahead of k tokens. We provide an implementation of an LL(1) parser.\n",
    "\n",
    "We first need to define a few tokens that will come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF = '\\0'\n",
    "EPSILON = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LL(1) grammars are rather restrictive. Specifically, the grammar should not contain left recursion. Hence, we have to\n",
    "update our original grammar to remove left-recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {'<start>': ['<expr>'],\n",
    "           '<expr>': ['<term><expr_>'],\n",
    "           '<expr_>': ['+<expr>',\n",
    "                       '-<expr>',\n",
    "                       ''],\n",
    "           '<term>': ['<factor><term_>'],\n",
    "           '<term_>': ['*<term>',\n",
    "                       '/<term>',\n",
    "                       ''],\n",
    "           '<factor>': ['+<factor>',\n",
    "                        '-<factor>',\n",
    "                        '(<expr>)',\n",
    "                        '<int>'],\n",
    "           '<int>': ['<integer><integer_>'],\n",
    "           '<integer_>': ['',\n",
    "                          '.<integer>'],\n",
    "           '<integer>': ['<digit><I>'],\n",
    "           '<I>': ['<integer>',\n",
    "                   ''],\n",
    "           '<digit>': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to change the grammar so that the productions become tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [split(e) for e in grammar[k]] for k in grammar}\n",
    "new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the listing of production rules, and the set of terminals in the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(g): return [(k, e) for k, a in g.items() for e in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminals(g):\n",
    "    return set(t for k, expr in rules(g) for t in expr if t not in g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and Follow sets\n",
    "\n",
    "\\todo{Define first and follow sets}\n",
    "We first define the fixpiont\n",
    "\\todo{Define what is a fixpoint of a function}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixpoint(f):\n",
    "    def helper(*args):\n",
    "        while True:\n",
    "            sargs = repr(args)\n",
    "            args_ = f(*args)\n",
    "            if repr(args_) == sargs:\n",
    "                return args\n",
    "            args = args_\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fixpoint\n",
    "def nullable_(rules, e):\n",
    "    for A, expression in rules:\n",
    "        if all((token in e)  for token in expression): e |= {A}\n",
    "    return (rules, e)\n",
    "\n",
    "def nullable(grammar):\n",
    "    return nullable_(rules(grammar), set())[1]\n",
    "\n",
    "\n",
    "@fixpoint\n",
    "def firstset_(rules, first, epsilon):\n",
    "    for A, expression in rules:\n",
    "        for token in expression:\n",
    "            first[A] |= first[token]\n",
    "\n",
    "            # update until the first token that is not nullable\n",
    "            if token not in epsilon:\n",
    "                break\n",
    "    return (rules, first, epsilon)\n",
    "\n",
    "def firstset(grammar, epsilon):\n",
    "    # https://www.cs.umd.edu/class/spring2014/cmsc430/lectures/lec05.pdf p6\n",
    "    # (1) If X is a terminal, then First(X) is just X\n",
    "    first = {i:{i} for i in terminals(grammar)}\n",
    "\n",
    "    # (2) if X ::= epsilon, then epsilon \\in First(X)\n",
    "    for k in grammar:\n",
    "        first[k] = {EPSILON} if k in epsilon else set()\n",
    "    return firstset_(rules(grammar), first, epsilon)[1]\n",
    "\n",
    "@fixpoint\n",
    "def followset_(grammar, epsilon, first, follow):\n",
    "    for A, expression in rules(grammar):\n",
    "        # https://www.cs.umd.edu/class/spring2014/cmsc430/lectures/lec05.pdf\n",
    "        # https://www.cs.uaf.edu/~cs331/notes/FirstFollow.pdf\n",
    "        # essentially, we start from the end of the expression. Then:\n",
    "        # (3) if there is a production A -> aB, then every thing in\n",
    "        # FOLLOW(A) is in FOLLOW(B)\n",
    "        # note: f_B serves as both follow and first.\n",
    "        f_B = follow[A]\n",
    "        for t in reversed(expression):\n",
    "            # update the follow for the current token. If this is the\n",
    "            # first iteration, then here is the assignment\n",
    "            if t in grammar:\n",
    "                follow[t] |= f_B  # only bother with nt\n",
    "\n",
    "            # computing the last follow symbols for each token t. This\n",
    "            # will be used in the next iteration. If current token is\n",
    "            # nullable, then previous follows can be a legal follow for\n",
    "            # next. Else, only the first of current token is legal follow\n",
    "            # essentially\n",
    "\n",
    "            # (2) if there is a production A -> aBb then everything in FIRST(B)\n",
    "            # except for epsilon is added to FOLLOW(B)\n",
    "            f_B = f_B | first[t] if t in epsilon else (first[t] - {EPSILON})\n",
    "\n",
    "    return (grammar, epsilon, first, follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followset(grammar, start):\n",
    "    # Initialize first and follow sets for non-terminals\n",
    "    follow = {i: set() for i in grammar}\n",
    "    follow[start] = {EOF}\n",
    "\n",
    "    epsilon = nullable(grammar)\n",
    "    first = firstset(grammar, epsilon)\n",
    "    return followset_(grammar, epsilon, first, follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnullable(rule, epsilon):\n",
    "    return all(token in epsilon for token in rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfirst(rule, first, epsilon):\n",
    "    tokens = set()\n",
    "    for token in rule:\n",
    "        tokens |= first[token]\n",
    "        if token not in epsilon: break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(rulepair, first, follow, epsilon):\n",
    "    A, rule = rulepair\n",
    "    rf = rfirst(rule, first, epsilon)\n",
    "    if rnullable(rule, epsilon):\n",
    "        rf |= follow[A]\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table(grammar, start, my_rules):\n",
    "    _, epsilon, first, follow = followset(grammar, start)\n",
    "\n",
    "    ptable = [(rule, predict(rule, first, follow, epsilon))\n",
    "              for rule in my_rules]\n",
    "\n",
    "    parse_tbl = {k: {} for k in grammar}\n",
    "\n",
    "    for (k, expr), pvals in ptable:\n",
    "        parse_tbl[k].update({v: (k, expr) for v in pvals})\n",
    "    return parse_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_helper(grammar, tbl, stack, inplst):\n",
    "    inp, *inplst = inplst\n",
    "    exprs = []\n",
    "    while stack:\n",
    "        val, *stack = stack\n",
    "        if isinstance(val, tuple):\n",
    "            exprs.append(val)\n",
    "        elif val not in grammar:  # terminal\n",
    "            assert val == inp\n",
    "            exprs.append(val)\n",
    "            inp, *inplst = inplst or [None]\n",
    "        else:\n",
    "            _, rhs = tbl[val][inp] if inp else (None, [])\n",
    "            stack = rhs + [(val, len(rhs))] + stack\n",
    "    return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(grammar, start, inp):\n",
    "    my_rules = rules(grammar)\n",
    "    parse_tbl = parse_table(grammar, start, my_rules)\n",
    "    k, _ = my_rules[0]\n",
    "    stack = [k]\n",
    "    return parse_helper(grammar, parse_tbl, stack, list(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_to_tree(arr):\n",
    "    stack = []\n",
    "    while arr:\n",
    "        elt = arr.pop(0)\n",
    "        if not isinstance(elt, tuple):\n",
    "            stack.append((elt, []))\n",
    "        else:\n",
    "            # get the last n\n",
    "            sym, n = elt\n",
    "            elts = stack[-n:] if n > 0 else []\n",
    "            stack = stack[0:len(stack) - n]\n",
    "            stack.append((sym, elts))\n",
    "    assert len(stack) == 1\n",
    "    return stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = linear_to_tree(parse(new_grammar, START_SYMBOL, '(1+2)*3'))\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earley parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(rule): return [i.strip() for i in rule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [shrink(split(e)) for e in EXPR_GRAMMAR[k]] for k in EXPR_GRAMMAR}\n",
    "new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fixpoint\n",
    "def nullable_(rules, e):\n",
    "    for A, expression in rules:\n",
    "        if all((token in e)  for token in expression): e |= {A}\n",
    "    return (rules, e)\n",
    "\n",
    "def nullable(grammar):\n",
    "    return nullable_(rules(grammar), set())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, name, expr, dot, origin, children=[]):\n",
    "        self.name, self.expr, self.dot, self.origin = name, expr, dot, origin\n",
    "        self.children = children[:]\n",
    "    def finished(self): return self.dot >= len(self.expr)\n",
    "    def shift(self):\n",
    "        return State(self.name, self.expr, self.dot+1, self.origin, self.children)\n",
    "    def symbol(self): return self.expr[self.dot]\n",
    "\n",
    "    def _t(self): return (self.name, self.expr, self.dot, self.origin.i, tuple(self.children))\n",
    "    def __hash__(self): return hash(self._t())\n",
    "    def __eq__(self, other): return  self._t() == other._t()\n",
    "\n",
    "class Column(object):\n",
    "    def __init__(self, i, token):\n",
    "        self.token, self.states, self._unique, self.i = token, [], {}, i\n",
    "\n",
    "    def add(self, state):\n",
    "        if state in self._unique: return self._unique[state]\n",
    "        self._unique[state] = state\n",
    "        self.states.append(state)\n",
    "        return self._unique[state]\n",
    "\n",
    "def predict(col, sym, grammar):\n",
    "    for alt in grammar[sym]:\n",
    "        col.add(State(sym, tuple(alt), 0, col))\n",
    "\n",
    "def scan(col, state, token):\n",
    "    if token == col.token:\n",
    "        col.add(state.shift())\n",
    "\n",
    "def complete(col, state, grammar):\n",
    "    for st in state.origin.states:\n",
    "        if st.finished(): continue\n",
    "        if state.name != st.symbol(): continue\n",
    "        col.add(st.shift()).children.append(state)\n",
    "\n",
    "# http://courses.washington.edu/ling571/ling571_fall_2010/slides/parsing_earley.pdf\n",
    "# https://github.com/tomerfiliba/tau/blob/master/earley3.py\n",
    "def parse(words, grammar, start):\n",
    "    # Aycock 2002 Practical Earley Parsing -- treatment of epsilon\n",
    "    epsilon = nullable(grammar)\n",
    "    alt = tuple(*grammar[start])\n",
    "    chart = [Column(i, tok) for i,tok in enumerate([None, *words])]\n",
    "    chart[0].add(State(start, alt, 0, chart[0], []))\n",
    "\n",
    "    for i, col in enumerate(chart):\n",
    "        for state in col.states:\n",
    "            if state.finished():\n",
    "                complete(col, state, grammar)\n",
    "            else:\n",
    "                sym = state.symbol()\n",
    "                if sym in grammar:\n",
    "                    predict(col, sym, grammar)\n",
    "                    if sym in epsilon:\n",
    "                        # note that precomputation of epsilon derivation can result in infinite\n",
    "                        # loops for certain grammars. Hence, we mark a nullable non-terminal\n",
    "                        # but do not expand it.\n",
    "                        col.add(state.shift()).children.append(State(sym + '*', tuple(), 0, col))\n",
    "                else:\n",
    "                    if i + 1 >= len(chart): continue\n",
    "                    scan(chart[i+1], state, sym)\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_expr(expr, children, grammar):\n",
    "    terms = iter([(i,[]) for i in expr if i not in grammar])\n",
    "    nts = iter([node_translator(i, grammar) for i in  children])\n",
    "    return [next(terms if i not in grammar else nts) for i in expr]\n",
    "\n",
    "def node_translator(state, grammar):\n",
    "    return (state.name, process_expr(state.expr, state.children, grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [shrink(split(e)) for e in EXPR_GRAMMAR[k]] for k in EXPR_GRAMMAR}\n",
    "table = parse(list('1+2+3'), new_grammar, '<start>')\n",
    "states = [st for st in table[-1].states if st.name == '<start>' and st.finished()]\n",
    "for state in states:\n",
    "    display_tree(node_translator(state, new_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguous grammars generates parse forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar= {\n",
    "        '<start>': ['<A>'],\n",
    "        '<A>': ['<A>+<A>', 'a'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [shrink(split(e)) for e in grammar[k]] for k in grammar}\n",
    "table = parse(list('a+a+a'), new_grammar, '<start>')\n",
    "states = [st for st in table[-1].states if st.name == '<start>' and st.finished()]\n",
    "for state in states:\n",
    "    display_tree(node_translator(state, new_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Lesson one_\n",
    "* _Lesson two_\n",
    "* _Lesson three_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _mutations_ on existing inputs to get more valid inputs](MutationFuzzer.ipynb)\n",
    "* [use _grammars_ (i.e., a specification of the input format) to get even more valid inputs](Grammars.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](Reducer.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Close the chapter with a few exercises such that people have things to do.  In Jupyter Notebook, use the `exercise2` nbextension to add solutions that can be interactively viewed or hidden:\n",
    "\n",
    "* Mark the _last_ cell of the exercise (this should be a _text_ cell) as well as _all_ cells of the solution.  (Use the `rubberband` nbextension and use Shift+Drag to mark multiple cells.)\n",
    "* Click on the `solution` button at the top.\n",
    "\n",
    "(Alternatively, just copy the exercise and solution cells below with their metadata.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "_Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "_Solution for the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
