{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Parsing and Recombining Inputs\n",
    "\n",
    "In the chapter on [Grammars](Grammars.ipynb), we discussed how grammars can be\n",
    "used to represent various languages. We also saw how grammars can be used to\n",
    "generate strings of the corresponding language. Grammars can also perform the\n",
    "reverse. That is, given a string, one can decompose the string into its\n",
    "constituent parts that correspond to the parts grammar used to generate it\n",
    "-- the derivation tree of that string. These parts (and parts from other similar\n",
    "strings) can later be recombined using the same grammar to produce new strings.\n",
    "\n",
    "In this chapter, we use grammars to parse and decompose inputs to\n",
    "their corresponding derivation trees, allowing us to recombine them\n",
    "arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb).\n",
    "* An understanding of derivation trees from the [chapter on grammar fuzzer](GrammarFuzzer.ipynb)\n",
    "  is also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzingbook_utils\n",
    "from Grammars import EXPR_GRAMMAR, START_SYMBOL, RE_NONTERMINAL\n",
    "from GrammarFuzzer import display_tree, all_terminals, GrammarFuzzer\n",
    "from ExpectError import ExpectError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to parse a string, one needs to identify the language, and the\n",
    "corresponding grammar. For example, here is a string that we would like to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = '1+2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This string is an arithmetic expression for addition, which may be specified using a grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_GRAMMAR = {\n",
    "   \"<start>\":\n",
    "       [\"<expr>\"],\n",
    "   \"<expr>\":\n",
    "       [\"<expr>+<expr>\", \"<expr>-<expr>\", \"<integer>\"],\n",
    "   \"<integer>\":\n",
    "       [\"<digit><integer>\", \"<digit>\"],\n",
    "   \"<digit>\":\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse tree for our expression from this grammar is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ('<start>',[\n",
    "    ('<expr>',[\n",
    "        ('<expr>',[('<integer>',[('<digit>',[('1',[])])])]),\n",
    "        ('+',[]),                           \n",
    "        ('<expr>',[('<integer>',[('<digit>',[('2',[])])])])])])\n",
    "assert mystring == all_terminals(tree)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a grammar can be used to specify a given language, there could be multiple\n",
    "grammars that correspond to the same language. For example, here is another \n",
    "grammar to describe the same addition expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2_GRAMMAR = {\n",
    "   \"<start>\":\n",
    "      [\"<expr>\"],\n",
    "   \"<expr>\":\n",
    "      [\"<integer><expr_>\"],\n",
    "   \"<expr_>\":\n",
    "      [\"+<expr>\", \"-<expr>\", \"\"],\n",
    "   \"<integer>\":\n",
    "      [\"<digit><integer_>\"],\n",
    "   \"<integer_>\":\n",
    "      [\"<integer>\", \"\"],\n",
    "   \"<digit>\":\n",
    "      [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding derivation tree is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ('<start>', [\n",
    "    ('<expr>', [('<integer>',\n",
    "                 [('<digit>', [('1', [])]), ('<integer_>', [('',[])])]),\n",
    "                ('<expr_>', [('+', []), \n",
    "                             ('<expr>', [\n",
    "                                 ('<integer>', [\n",
    "                                     ('<digit>', [('2', [])]),\n",
    "                                     ('<integer_>', [('',[])])]),\n",
    "                                 ('<expr_>', [('',[])])])])])])\n",
    "assert mystring == all_terminals(tree)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there could be different classes of grammars that\n",
    "describe the same language. For example, the first grammar `A1_GRAMMAR`\n",
    "is a grammar that sports both _right_ and _left_ recursion, while the\n",
    "second grammar `A2_GRAMMAR` does not have left recursion in the\n",
    "non-terminals in any of its productions, but contains _epsilon_ productions.\n",
    "(An epsilon production is a production that has empty string in its right\n",
    "hand side.)\n",
    "\n",
    "A grammar is left recursive if any of its non-terminals are left recursive,\n",
    "and a non-terminal is directly left-recursive if the left-most symbol of\n",
    "any of its productions is itself. It is indirectly left-recursive if any\n",
    "of the left-most symbols can be expanded using their definitions to\n",
    "produce the non-terminal as the left-most symbol of the expansion.\n",
    "Right recursive grammars are defined similarly.\n",
    "\n",
    "For example, in `A1_GRAMMAR`, the definition of `<expr>` is\n",
    "left-recursive, and right recursive directly. However in `A2_GRAMMAR`,\n",
    "`<expr>` is right recursive indirectly through the expansion of `<expr_>`.\n",
    "\n",
    "To complicate matters further, there could be\n",
    "multiple derivation trees -- also called _parses_ -- corresponding to the\n",
    "same string from the same grammar. For example, a string `1+2+3` can be parsed\n",
    "in two ways as we see below using the `A1_GRAMMAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = '1+2+3'\n",
    "tree = ('<start>', [\n",
    "    ('<expr>', [\n",
    "        ('<expr>', [\n",
    "            ('<expr>', [('<integer>', [('<digit>', [('1', [])])])]),\n",
    "            ('+', []),\n",
    "            ('<expr>', [('<integer>', [('<digit>', [('2', [])])])])]), \n",
    "        ('+', []), \n",
    "        ('<expr>', [('<integer>', [('<digit>', [('3', [])])])])])])\n",
    "assert mystring == all_terminals(tree)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ('<start>', [\n",
    "    ('<expr>', [\n",
    "        ('<expr>', [('<integer>', [('<digit>', [('1', [])])])]), \n",
    "        ('+', []), \n",
    "        ('<expr>', [\n",
    "            ('<expr>', [('<integer>', [('<digit>', [('2', [])])])]), \n",
    "            ('+', []), \n",
    "            ('<expr>', [('<integer>', [('<digit>', [('3', [])])])])]) ])])\n",
    "assert all_terminals(tree) == mystring\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous parsing techniques exist that can take a grammar and the given\n",
    "string, and produce the corresponding derivation tree or trees. However,\n",
    "some of these techniques work only on specific classes of grammars.\n",
    "These classes of grammars are named after the specific kind of parser\n",
    "that can accept grammars of that category. That is, the upper bound for\n",
    "the capabilities of the parser defines the grammar class named after that\n",
    "parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different classes of grammars differ in the features that are available to\n",
    "the user for writing a grammar of that class. That is, the corresponding\n",
    "kind of parser will be unable to parse a grammar that makes use of more\n",
    "features than is allowed. For example, the `A2_GRAMMAR` is an _LL(1)_\n",
    "grammar because it lacks left recursion, while `A1_GRAMMAR` is not an\n",
    "_LL(1)_ grammar. This is because an _LL_ parser is a parser that parses\n",
    "its input from left to right, and constructs a leftmost derivation of its\n",
    "input by expanding the non-terminals it encounters. If there is a left\n",
    "recursion in one of these rules, an _LL_ parser will enter an infinite loop.\n",
    "\n",
    "We will examine a few classes of parsers next. First we define a few common\n",
    "features required by all parsers we implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce, lru_cache\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `EXPR_GRAMMAR` we import from the [chapter on grammars](Grammars.ipynb) is oriented towards generation. In particular, the production rules are stored as strings. We need to massage this representation a little to conform to a canonical representation where each token in a rule is represented separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(rule):\n",
    "    return [token\n",
    "            for token in re.split(RE_NONTERMINAL, rule)\n",
    "            if token]\n",
    "\n",
    "def canonical(grammar):\n",
    "    return  {key: [split(choice) for choice in choices]\n",
    "             for key, choices in grammar.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a minimal interface for parsing, that is obeyed by all parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    def __init__(self, grammar, start_symbol=START_SYMBOL):\n",
    "        self.start_symbol = start_symbol\n",
    "        self.grammar = grammar\n",
    "        \n",
    "    def parse_prefix(self, text):\n",
    "        \"\"\"Return pair (cursor, forest) for longest prefix of text\"\"\"\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def parse(self, text):\n",
    "        cursor, forest = self.parse_prefix(text)\n",
    "        if cursor < len(text):\n",
    "            raise SyntaxError(\"at \" + repr(text[cursor:]))\n",
    "        return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Expression Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short of hand rolling a parser, _Packrat_ parsing is one of the simplest parsing techniques, and the class of grammar it accepts is called a _Parsing Expression Grammar_ (_Packrat_ is one of the techniques for parsing PEGs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _Parsing Expression Grammar_ comes is very similar to a _Context Free Grammar_. As in a _Context Free Grammar_, the grammar is represented by a set of non-terminals and corresponding **choice expressions** representing how to match each. For example, here is a PEG that matches `a` or `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peg = {\n",
    "    '<start>': [['a'],['b']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, unlike the _CFG_, the choice expression is an **ordered choice**. That is, rather than choosing all rules that can potentially match, we stop at the first match that succeed. For example, the below _PEG_ can match `ab` but not `abc` unlike a _CFG_ which will match both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peg = {\n",
    "    '<start>': [['ab'],['abc']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each choice in a _choice expression_ represents a rule on how to satisfy that particular choice. The choice is a sequence of tokens (terminals and non-terminals) that are matched against a given text as in a _CFG_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the syntax of grammar definitions we have seen so far, a _PEG_ can can also contain a few additional elements. See the exercises at the end of the chapter for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsing expression grammars model the typical practice in handwritten recursive descent parsers, and hence it may be considered more intuitive to understand. Further, it comes with attractive properties such as linear time parsing (the time required to parse increases linearly with the increase in input length).\n",
    "\n",
    "We look at the implementation of a simple PEG parser next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packrat Parser for _PEGs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We derive from the `Parser` base class first, and we accept the text to be parsed in the `parse` method, which in turn calls `unify_key` with the `start_symbol` which is the starting point of parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEGParser(Parser):\n",
    "    def __init__(self, grammar, start_symbol):\n",
    "        super().__init__(canonical(grammar), start_symbol)\n",
    "        \n",
    "    def parse_prefix(self, text):\n",
    "        return self.unify_key(self.start_symbol, text, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unify_key` algorithm is simple. If given a terminal symbol, it tries to match the symbol with the current position in the text. If the symbol and text match, it returns successfully with the new parse index `at`.\n",
    "\n",
    "If on the other hand, it was given a non-terminal, it retrieves the choice expression corresponding to the key, and tries to match each choice in order using `unify_rule`. If **any** of the rules succeed in being unified with the given text, the parse is considered a success, and we return with the new parse index returned by `unify_rule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser(PEGParser):\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if key not in self.grammar:\n",
    "            if text[at:].startswith(key): return at + len(key), (key, [])\n",
    "            else: return at, None\n",
    "        for rule in self.grammar[key]:\n",
    "            to, res = self.unify_rule(rule, text, at)\n",
    "            if res: return (to, (key, res))\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unify_rule` is similar. It retrieves the tokens corresponding to the rule that it needs to unify with the text, and calls `unify_key` on them in sequence. If **all** tokens are successfully unified with the text, the parse is a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEGParser(PEGParser):\n",
    "    def unify_rule(self, rule, text, at):\n",
    "        results = []\n",
    "        for token in rule:\n",
    "            at, res = self.unify_key(token, text, at)\n",
    "            if res is None: return at, None\n",
    "            results.append(res)\n",
    "        return at, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods are mutually recursive, and given that `unify_key` tries each alternative until it succeeds, `unify_key` can be called multiple times with the same arguments. Hence, it is import to memoize the results of `unify_key`. Python provides a simple decorator that is available at `functools.lru_cache` for memoizing any function call that has hashable arguments. We add that to our implementation so that repeated calls to `unify_key` with same arguments get cached results.\n",
    "\n",
    "This memoization gives the algorithm its name -- _Packrat_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PEGParser(PEGParser):\n",
    "    @lru_cache(maxsize=None)\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if key not in self.grammar:\n",
    "            if text[at:].startswith(key): return at + len(key), (key, [])\n",
    "            else: return at, None\n",
    "        for rule in self.grammar[key]:\n",
    "            to, res = self.unify_rule(rule, text, at)\n",
    "            if res: return (to, (key, res))\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap initialization and calling of `PEGParser` in a method `parse` that accepts the text to be parsed along with the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text, grammar):\n",
    "    peg = PEGParser(grammar, START_SYMBOL)\n",
    "    return peg.parse(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of our parser in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = \"1 + (2 * 3)\"\n",
    "tree = parse(mystring, EXPR_GRAMMAR)\n",
    "assert all_terminals(tree) == mystring\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = \"1 * (2 + 3.35)\"\n",
    "tree = parse(mystring, EXPR_GRAMMAR)\n",
    "assert all_terminals(tree) == mystring\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should be aware that while the grammar looks like a _Context Free Grammar_, the language described by a PEG may be different (only _LL(1)_ grammars are guaranteed to represent the same language for both PEGs and other parsers), while other behaviors could be surprising~\\cite{redziejowski2008}. \n",
    "\n",
    "While _Parsing Expression Grammars_ are simple at first sight, their behavior in some cases might be a bit unintuitive. For example, here is an example from Redziejowski~\\cite{redziejowski}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEG_SURPRISE = {\n",
    "    \"<A>\": [\"a<A>a\",\"aa\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreted as a context free grammar and used as a string generator, it will produce strings of the form `a, aa, aaaa, aaaaaa` that is, it produces strings where the number of `a` is \\Latex{2*n}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = []\n",
    "for e in range(4):\n",
    "    f = GrammarFuzzer(PEG_SURPRISE, '<A>')\n",
    "    tree = ('<A>',None)\n",
    "    for _ in range(e):\n",
    "        tree = f.expand_tree_once(tree)\n",
    "    tree = f.expand_tree_with_strategy(tree, f.expand_node_min_cost)\n",
    "    strings.append(all_terminals(tree))\n",
    "    display_tree(tree)\n",
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peg = PEGParser(PEG_SURPRISE, '<A>')\n",
    "for s in strings:\n",
    "    with ExpectError():\n",
    "        tree = peg.parse(s)\n",
    "        display_tree(tree)\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Free Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the _Parsing Expression Grammars_ are expressive and the _packrat_ parser for parsing them is simple and intuitive, PEGs suffer from a major deficiency for our purposes. PEGs are oriented towards language recognition, and it is not clear how to translate an arbitrary PEG to a _CFG_. As we mentioned earlier, a naive re-interpretation of a PEG as a CFG does not work very well. Further, it is not clear what is the exact relation between the class of languages represented by _PEG_ and the the class of languages represented by _CFG_. Since our primary focus is _fuzzing_ -- that is _generation_ of strings, we next look at parsers that can accept _CFG_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require meta characters for marking an empty string, and for marking the end of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF = '\\0'\n",
    "EPSILON = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rules` method takes in a grammar and returns `(key,production)` pairs for each production in the grammar. This is an alternative representation of a grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(grammar):\n",
    "    return [(key, choice)\n",
    "            for key, choices in grammar.items()\n",
    "            for choice in choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules(canonical(EXPR_GRAMMAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `terminals` method extracts all terminal symbols from a `canonical` grammar representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminals(grammar):\n",
    "    return set(token\n",
    "               for key, choice in rules(grammar)\n",
    "               for token in choice if token not in grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminals(canonical(EXPR_GRAMMAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing, one often requires to know whether a given non-terminal can derive an empty string. For example, in the following grammar, the rule for `A` can derive an empty string, while the rule for `B` can't. The non-terminals that can derive an empty string is called a `nullable` non-terminal, and the set of all such non-terminals is called a `nullable` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grammar = {\n",
    "    '<start>': [['<A>'], ['<B>']],\n",
    "    '<A>': [['a'],['']],\n",
    "    '<B>': [['b']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the `nullable` set requires expanding each production rule in the grammar iteratively and inspecting whether a given rule can derive the empty string. Each iteration needs to take into account new terminals that have been found to be nullable. The procedure stops when we obtain a stable result. This procedure can be abstracted into a more general method `fixpoint`. A `fixpoint` of a function is an element in the function's domain such that it is mapped to itself. For example, `1` is a `fixpoint` of `math.sqrt` because `math.sqrt(1.0) == 1.0`. \n",
    "\n",
    "(We use `str` rather than `hash` to check for equality in `fixpoint` because `set`, which we would like to use as an argument has a good string representation but is not hashable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixpoint(f):\n",
    "    def helper(arg):\n",
    "        while True:\n",
    "            sarg = str(arg)\n",
    "            arg_ = f(arg)\n",
    "            if str(arg_) == sarg:\n",
    "                return arg\n",
    "            arg = arg_\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember `my_sqrt` from the first chapter? We can define `my_sqrt` using `fixpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sqrt(x):\n",
    "    @fixpoint\n",
    "    def _my_sqrt(approx):\n",
    "        return (approx + x / approx) / 2\n",
    "    return _my_sqrt(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can define `nullable` using `fixpoint`. We essentially provide the definition of a single intermediate step. That is, assuming that `nullables` contain the current nullable non-terminals, we iterate over the grammar looking for productions which are `nullable` -- that is, productions where the entire sequence can yield an empty string on some expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullable(grammar):\n",
    "    productions = rules(grammar)\n",
    "    @fixpoint\n",
    "    def nullable_(nullables):\n",
    "        for A, expr in productions:\n",
    "            if all(token in nullables for token in expr):\n",
    "                nullables |= {A}\n",
    "        return (nullables)\n",
    "    return nullable_({EPSILON})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nullable non-terminals in `my_grammar` is computed as expected (It includes EPSILON which is technically not a non-terminal, but you can ignore that for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullable(my_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (**Advanced**: This `nullable` implementation, while simple, is not the best. See [Jeffrey Kegler](https://github.com/jeffreykegler/kollos/blob/master/notes/misc/loup2.md) for a faster implementation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require an additional method to massage our `EXPR_GRAMMAR`. This method removes the extraneous spaces from the grammar definition. While the spaces are reasonable when one is generating strings, they make our parses needlessly complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(rule):\n",
    "    return [i.strip() for i in rule]\n",
    "\n",
    "def canonical(grammar):\n",
    "    return  {k: [shrink(split(l)) for l in rules]\n",
    "             for k, rules in grammar.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earley parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Earley parser is a general parser that is able to parse arbitrary _Context Free Grammars_. It was invented by Jay Earley~\\cite{Earley1970} for use in computational linguistics. While its computational complexity is $O(n^3)$ for arbitrary grammars, it can parse unambiguous grammars in $O(n^2)$ time, and all _[LR(k)](https://en.wikipedia.org/wiki/LR_parser)_ grammars in linear time ($O(n)$~\\cite{Leo1991}). Further improvements -- notably handling epsilon rules -- were invented by Aycock et al.~\\cite{Aycock2002}.\n",
    "\n",
    "Note that one restriction of our implementation is that the start symbol should have only one choice in its choice expression. This is not a restriction in practice because any grammar with multiple choices in its start symbol can be extended with a new start symbol that has the original start symbol as its only choice. That is, given a grammar as below to conform to the single choice rule:\n",
    "\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': [['<A>'], ['<B>']],\n",
    "    '<A>': [['a'], ['A']],\n",
    "    '<B>': [['b'], ['']],    \n",
    "}\n",
    "```\n",
    "One may rewrite it as\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': [['<start_>']],\n",
    "    '<start_>': [['<A>'], ['<B>']],\n",
    "    '<A>': [['a'], ['A']],\n",
    "    '<B>': [['b'], ['']],    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser uses dynamic programming to generate a table containing a forest of possible parses at each letter index -- the table contains as many columns as there are letters in the input, and each column contains different parsing rules at various stages of the parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns\n",
    "We define the `Column` first. The `Column` is initialized by its own `index` in the input string, and the `letter` at that index. Internally, we also keep track of the states that are added to the column as the parsing progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Column(object):\n",
    "    def __init__(self, index, letter):\n",
    "        self.index, self.letter = index, letter\n",
    "        self.states, self._unique =  [], {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Column` only stores unique `states`. Hence, when a new `state` is `added` to our `Column`, we check whether it is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Column(Column):\n",
    "    def add(self, state):\n",
    "        if state in self._unique: return self._unique[state]\n",
    "        self._unique[state] = state\n",
    "        self.states.append(state)\n",
    "        return self._unique[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### States\n",
    "\n",
    "A `State` represents a parse in progress for a specific rule corresponding to a non-terminal. Hence, the state contains the `name` of the non-terminal, the corresponding choice expression (`expr`), and the current position of parsing in this expression (`dot`). We further store the starting column `s_col` and ending column `e_col` for each. Finally, we store a references to all the child states (that is, all states that originates from non-terminals in the `expr`). Since we are interested in comparing states, we define `hash` and `eq` with the corresponding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, name, expr, dot, s_col, children=[]):\n",
    "        self.name, self.expr, self.dot = name, expr, dot\n",
    "        self.s_col, self.e_col = s_col, None\n",
    "        self.children = children[:]\n",
    "        \n",
    "    def _t(self):\n",
    "        return (self.name, self.expr, self.dot, self.s_col.index, tuple(self.children))\n",
    "    def __hash__(self): return hash(self._t())\n",
    "    def __eq__(self, other): return  self._t() == other._t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a few convenience methods. The method `finished` checks if the `dot` has moved beyond the last element in `expr`. The method `advance` produces a new `State` with the `dot` advanced one token, and represents an advance of the parse state. and `at_dot` returns the current symbol being parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(State):\n",
    "    def finished(self):\n",
    "        return self.dot >= len(self.expr)\n",
    "    def advance(self):\n",
    "        return State(self.name, self.expr, self.dot+1, self.s_col, self.children)\n",
    "    def at_dot(self):\n",
    "        return self.expr[self.dot] if self.dot < len(self.expr) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Parser\n",
    "\n",
    "We begin the parse implementation with the static initialization of `nullable` set that we defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(Parser):\n",
    "    def __init__(self, grammar, start_symbol):\n",
    "        super().__init__(grammar, start_symbol)\n",
    "        self.epsilon = nullable(self.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _Earley_ algorithm starts by initializing the chart with columns (as many as there are letters in the input). We also seed the first column with a state representing the choice expression corresponding to the start symbol. We pass this partial chart to a method for filling the rest of the parse chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def chart_parse(self, words, start):\n",
    "        alt = tuple(*self.grammar[start])\n",
    "        chart = [Column(i, tok) for i,tok in enumerate([None, *words])]\n",
    "        chart[0].add(State(start, alt, 0, chart[0], []))\n",
    "        return self.fill_chart(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop has three fundamental operations. We have seeded the chart with the state representing parse of the start symbol. The choice expression corresponding to the start symbol may be as follows:\n",
    "\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': [['<A>','b']],\n",
    "    <A>: [['a'], ['A']],\n",
    "}\n",
    "```\n",
    "\n",
    "##### Predict\n",
    "\n",
    "We have already seeded `chart[0]` with a state `[<A>,b]` with `dot` at `0`. Next, given that `<A>` is a non-terminal, we `predict` the possible parse continuations of this state. That is, it could be either `a` or `A`.\n",
    "\n",
    "In essence, the `predict` method, when called with the current non-terminal (`<A>` in this case), fetches the choices expressions corresponding to this non-terminal, and adds these as predicted _child_ states to the _current_ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def predict(self, col, sym):\n",
    "        for alt in self.grammar[sym]:\n",
    "            col.add(State(sym, tuple(alt), 0, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scan\n",
    "What if rather than a non-terminal, the state contained a terminal symbol -- i.e a letter? In that case, we are ready to make some progress. We `scan` the next column's letter. If the letter matches what we have, then create a new state by advancing the current state by one letter, and add it to the next column (i.e the column that has the matched letter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def scan(self, col, state, letter):\n",
    "        if letter == col.letter:\n",
    "            col.add(state.advance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete\n",
    "\n",
    "When we advance, what if we actually `complete` the processing of the current rule? If so, we want to update not just this state, but also all the _parent_ states from which this state was derived.\n",
    "\n",
    "How do we determine the parent states? Note from `predict` that we added the predicted child states to the _same_ column as that of the inspected state? Hence, we look at the starting column of the current state, with the same symbol `at_dot` as that of the name of the completed state.\n",
    "\n",
    "For each such parent found, we advance that parent (because we have just finished parsing that non terminal for their `at_dot`) and add the new states to the current column. We also mark the current state as a child state for the advanced parent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def complete(self, col, state): return self.earley_complete(col, state)\n",
    "    def earley_complete(self, col, state):\n",
    "        parent_states = [st for st in state.s_col.states\n",
    "                     if st.at_dot() == state.name]\n",
    "        for st in parent_states:\n",
    "            col.add(st.advance()).children.append(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill chart\n",
    "The main driving loop in `fill_chart` essentially calls these operations in order. We loop over each column in order. For each column, fetch one state in the column at a time, and check if the state is `finished`. If it is, then we `complete` all the parent states depending on this state. If the state was not finished, we check to see if the state's current symbol `at_dot` is a non-terminal. If it is a non-terminal, we `predict` possible continuations, and update the current column with these states. If it was not, we `scan` the next column and advance the current state if it matches the next letter.\n",
    "\n",
    "The original implementation of the algorithm by Jay Earley did not handle rules that can derive empty strings (these are called _epsilon rules_). Aycock et al. provided a simple solution for handling epsilon rules. If one is of the symbols during the `predict` step is in the `nullable` set, simply advance the state and add it to the current column.\n",
    "\n",
    "This new state represents completion of the current state (by deriving an empty string). However, the derivation of empty string may not be direct. It may represent a number of derivation steps, each of which can add new child states. Hence, we add a marker to the state's children indicating that such derivations can exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def fill_chart(self, chart):\n",
    "        for i, col in enumerate(chart):\n",
    "            for state in col.states:\n",
    "                if state.finished():\n",
    "                    self.complete(col, state)\n",
    "                else:\n",
    "                    sym = state.at_dot()\n",
    "                    if sym in self.grammar:\n",
    "                        self.predict(col, sym)\n",
    "                        if sym in self.epsilon:\n",
    "                            c = col.add(state.advance())\n",
    "                            c.children.append(State(sym + '*', tuple(), 0, col))\n",
    "                    else:\n",
    "                        if i + 1 >= len(chart): continue\n",
    "                        self.scan(chart[i+1], state, sym)\n",
    "        return chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `chart_parse` returns the completed table, we now need to extract the derivation trees. Fortunately, this is simple given that we have kept the list of children for each state. All that we need to do is to fetch the finished states corresponding to the `start_symbol`, and retrieve the derivation trees for each child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def parse(self, text):\n",
    "        table = self.chart_parse(text, self.start_symbol)\n",
    "        states = [st for st in table[-1].states if st.name == self.start_symbol and st.finished()]\n",
    "        for state in states:\n",
    "            yield self.derivation_tree(state)\n",
    "\n",
    "    def process_expr(self, expr, children):\n",
    "        terms = iter([(i,[]) for i in expr if i not in self.grammar])\n",
    "        nts = iter([self.derivation_tree(i) for i in  children])\n",
    "        return [next(terms if i not in self.grammar else nts) for i in expr]\n",
    "\n",
    "    def derivation_tree(self, state):\n",
    "        return (state.name, self.process_expr(state.expr, state.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text, grammar, start=START_SYMBOL):\n",
    "    ep = EarleyParser(grammar, start)\n",
    "    return ep.parse(text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = canonical(EXPR_GRAMMAR)\n",
    "for tree in parse(list('1+2+3'), new_grammar):\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguous grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambiguous grammars generate parse forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar= {\n",
    "        '<start>': ['<A>'],\n",
    "        '<A>': ['<A>+<A>', 'a'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = canonical(grammar)\n",
    "for tree in parse(list('a+a+a'), new_grammar, '<start>'):\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Information\n",
    "* Parsing with arbitrary context free grammars is reducible to boolean matrix multiplication~\\cite{Valiant1975} (and the reverse~\\cite{Lee2002}) which is at present bounded by \\Latex{O(2^{23728639}}) ~\\cite{LeGall2014}\n",
    "* The actual class of languages that is expressible in PEG is currently unknown. In particular, we know that PEGs can express certain languages such as \\Latex{a^n b^n c^n}. However, we do not know if there exist Context-Free languages that are not expressible with PEGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Lesson one_\n",
    "* _Lesson two_\n",
    "* _Lesson three_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _mutations_ on existing inputs to get more valid inputs](MutationFuzzer.ipynb)\n",
    "* [use _grammars_ (i.e., a specification of the input format) to get even more valid inputs](Grammars.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](Reducer.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Close the chapter with a few exercises such that people have things to do.  In Jupyter Notebook, use the `exercise2` nbextension to add solutions that can be interactively viewed or hidden:\n",
    "\n",
    "* Mark the _last_ cell of the exercise (this should be a _text_ cell) as well as _all_ cells of the solution.  (Use the `rubberband` nbextension and use Shift+Drag to mark multiple cells.)\n",
    "* Click on the `solution` button at the top.\n",
    "\n",
    "(Alternatively, just copy the exercise and solution cells below with their metadata.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "The _PEG_ syntax provides a few notational conveniences reminiscent of regular expressions. For example, it supports the following operators (letters `T` and `A` represents tokens that can be either terminal or non-terminal. `ε` is an empty string, and `/` is the ordered choice operator similar to the non-ordered choice operator `|`):\n",
    "\n",
    "* `T?` represents an optional greedy match of T and `A := T?` is equivalent to `A := T/ε`.\n",
    "* `T*` represents zero or more greedy matches of `T` and `A := T*` is equivalent to `A := T A/ε`.\n",
    "* `T+` represents one or more greedy matches -- equivalent to `TT*`\n",
    "\n",
    "If you look at the three notations above, each can be represented in the grammar in terms of basic syntax.\n",
    "Remember the exercise from [the chapter on grammars](Grammars.ipynb) that developed `define_ex_grammar` that can represent grammars as Python code? extend `define_ex_grammar` to `define_peg` to support the above notational conveniences. The decorator should rewrite a given grammar that contain these notations to an equivalent grammar in basic syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Beyond these notational conveniences, it also supports two predicates that can provide a powerful lookahead facility that does not consume any input.\n",
    "\n",
    "* `T&A` represents an _And-predicate_ that matches `T` if `T` is matched, and it is immediately followed by `A`\n",
    "* `T!A` represents a _Not-predicate_ that matches `T` if `T` is matched, and it is *not* immediately followed by `A`\n",
    "\n",
    "Implement these predicates in our _PEG_ parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "_No solution yet_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "In the `Earley Parser`, `Column` class, we keep the states both as a `list` and also as a `dict` even though `dict` is ordered. Can you explain why?\n",
    "\n",
    "**Hint**: see the `fill_chart` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "In the `Earley Parser` explanation for `fill_chart` method, we added a marker indicating an epsilon derivation. Can one statically compute such epsilon derivations given a grammar? can one do it exhaustively given any CFG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 5\n",
    "\n",
    "In `Earley Parser` methods, we kept a list of children for each state. However, if you see `complete`, we computed the parent states directly from a state. Given that we can obtain leaf states directly from the last column, can you extract the parse forest directly without keeping a list of children per state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_NONTERMINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 6\n",
    "\n",
    "The `Earley Parser` as it is implemented, has a computational complexity of $O(n^2)$ for _LR_ grammars. Implement the Leo Joop optimization to make it linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeoParser(EarleyParser):\n",
    "    def check_single_item(self, st, remain):\n",
    "        res = [s for s in remain\n",
    "                 if s.name == st.name and s.expr == st.expr and\n",
    "                    s.s_col.i == st.s_col.i and s.dot == (st.dot - 1)]\n",
    "        return len(res) == 1\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_above(self, state):\n",
    "        remain, finished = splitlst(lambda s: s.finished(), state.s_col.states)\n",
    "        vals = [st for st in finished\n",
    "                    if state.name == st.expr[-1] and\n",
    "                       self.check_single_item(st, remain)]\n",
    "        if vals:\n",
    "            assert len(vals) == 1\n",
    "            res = self.get_above(vals[0])\n",
    "            return vals[0] if not res else res\n",
    "        return None\n",
    "\n",
    "    def leo_complete(self, col, state):\n",
    "        detred = self.get_above(state)\n",
    "        if detred:\n",
    "            col.add(detred.copy()).children.append(state)\n",
    "        else:\n",
    "            self.earley_complete(col, state)\n",
    "\n",
    "    def complete(self, col, state): return self.leo_complete(col, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above code does not handle epsilons very well. Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
