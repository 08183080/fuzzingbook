{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Parsing and Recombining Inputs\n",
    "\n",
    "In the chapter on [Grammars](Grammars.ipynb), we discussed how grammars can be\n",
    "used to represent various languages. We also saw how grammars can be used to\n",
    "generate strings of the corresponding language. Grammars can also perform the\n",
    "reverse. That is, given a string, one can decompose the string into its\n",
    "constituent parts that correspond to the parts grammar used to generate it\n",
    "-- the derivation tree of that string. These parts (and parts from other similar\n",
    "strings) can later be recombined using the same grammar to produce new strings.\n",
    "\n",
    "In this chapter, we use grammars to parse and decompose inputs to\n",
    "their corresponding derivation trees, allowing us to recombine them\n",
    "arbitrarily.\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb).\n",
    "* An understanding of derivation trees from the [chapter on grammar fuzzer](GrammarFuzzer.ipynb)\n",
    "  is also required.\n",
    "\n",
    "In order to parse a string, one needs to identify the language, and the\n",
    "corresponding grammar. For example, here is a string that we would like to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = '1+2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This string is an arithmetic expression for addition, which may be specified using a grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_GRAMMAR = {\n",
    "   \"<start>\":\n",
    "       [\"<expr>\"],\n",
    "   \"<expr>\":\n",
    "       [\"<expr>+<expr>\", \"<expr>-<expr>\", \"<integer>\"],\n",
    "   \"<integer>\":\n",
    "       [\"<digit><integer>\", \"<digit>\"],\n",
    "   \"<digit>\":\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse tree for our expression from this grammar is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(('<start>',[('<expr>',[('<expr>',[('<integer>',[('<digit>',[('1',[])])])]),('+',[]),('<expr>',[('<integer>',[('<digit>',[('2',[])])])])])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a grammar can be used to specify a given language, there could be multiple\n",
    "grammars that correspond to the same language. For example, here is another \n",
    "grammar to describe the same addition expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2_GRAMMAR = {\n",
    "   \"<start>\":\n",
    "      [\"<expr>\"],\n",
    "   \"<expr>\":\n",
    "      [\"<integer><expr_>\"],\n",
    "   \"<expr_>\":\n",
    "      [\"+<expr>\", \"-<expr>\", \"\"],\n",
    "   \"<integer>\":\n",
    "      [\"<digit><integer_>\"],\n",
    "   \"<integer_>\":\n",
    "      [\"<integer>\", \"\"],\n",
    "   \"<digit>\":\n",
    "      [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding derivation is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " display_tree(('<start>', [('<expr>', [('<integer>', [('<digit>', [('1', [])]), ('<integer_>', [])]), ('<expr_>', [('+', []), ('<expr>', [('<integer>', [('<digit>', [('2', [])]), ('<integer_>', [])]), ('<expr_>', [])])])])])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there could be different classes of grammars that\n",
    "describe the same language. For example, the first grammar `A1_GRAMMAR`\n",
    "is a grammar that sports both _right_ and _left_ recursion, while the\n",
    "second grammar `A2_GRAMMAR` does not have left recursion in the\n",
    "non-terminals in any of its productions, but contains _epsilon_ productions.\n",
    "(An epsilon production is a production that has empty string in its right\n",
    "hand side.)\n",
    "\n",
    "A grammar is left recursive if any of its non-terminals are left recursive,\n",
    "and a non-terminal is directly left-recursive if the left-most symbol of\n",
    "any of its productions is itself. It is indirectly left-recursive if any\n",
    "of the left-most symbols can be expanded using their definitions to\n",
    "produce the non-terminal as the left-most symbol of the expansion.\n",
    "Right recursive grammars are defined similarly.\n",
    "\n",
    "For example, in `A1_GRAMMAR`, the definition of `<expr>` is\n",
    "left-recursive, and right recursive directly. However in `A2_GRAMMAR`,\n",
    "`<expr>` is right recursive indirectly through the expansion of `<expr_>`.\n",
    "\n",
    "To complicate matters further, there could be\n",
    "multiple derivation trees -- also called _parses_ -- corresponding to the\n",
    "same string from the same grammar. For example, a string `1+2+3` can be\n",
    "parsed either as `{expr: {expr: 1+2}+3}` or  as `{expr: 1+{expr: 2+3}}`.\n",
    "\n",
    "Numerous parsing techniques exist that can take a grammar, and the given\n",
    "string, and produce the corresponding derivation tree or trees. However,\n",
    "some of them work only on specific classes of grammars. These classes of\n",
    "grammars are named after the specific kind of parser that can accept\n",
    "grammars of that category.\n",
    "\n",
    "Different classes of grammars differ in the features that are available to\n",
    "the user for writing a grammar of that class. That is, the corresponding\n",
    "kind of parser will be unable to parse a grammar that makes use of more\n",
    "features than is allowed. For example, the `A2_GRAMMAR` is an _LL(1)_\n",
    "grammar because it lacks left recursion, while `A1_GRAMMAR` is not.\n",
    "This is because an _LL(k)_ parser is a parser that parses its input from\n",
    "left to right, and constructs a leftmost derivation of its input using *k*\n",
    "lookahead tokens.\n",
    "\n",
    "We will examine a few classes of parsers next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize a few things required by our parsing infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzingbook_utils\n",
    "from Grammars import EXPR_GRAMMAR, START_SYMBOL, RE_NONTERMINAL\n",
    "from GrammarFuzzer import display_tree\n",
    "import functools\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `EXPR_GRAMMAR` we import from the [chapter on grammars](Grammars.ipynb) is oriented towards generation. In particular, the production rules are stored as strings. We need to massage this representation a little to conform to a canonical representation where each token in a rule is represented separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(rule):\n",
    "    return [s for s in re.split(RE_NONTERMINAL, rule) if s]\n",
    "\n",
    "def canonical(grammar):\n",
    "    return  {k: [split(l) for l in rules] for k, rules in grammar.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a minimal interface for parsing, that is obeyed by all parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    def __init__(self, grammar, start_symbol=START_SYMBOL):\n",
    "        self.start_symbol = start_symbol\n",
    "        self.grammar = grammar\n",
    "        \n",
    "    def parse_prefix(self, text):\n",
    "        \"\"\"Return pair (cursor, forest) for longest prefix of text\"\"\"\n",
    "        raise NotImplemented()\n",
    "        \n",
    "    def parse(self, text):\n",
    "        cursor, forest = self.parse_prefix(text)\n",
    "        if cursor < len(text):\n",
    "            raise SyntaxError(\"at \" + repr(text[cursor:]))\n",
    "        return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packrat Parsing for _Parsing Expression Grammars_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short of handrolling a parser, _Packrat_ parsing is one of the simplest parsing techniques, and the class of grammar it accepts is called a _Parsing Expression Grammar_ (Packrat is one of the techniques for parsing PEGs). The parsing expression grammars model the typical practice in handwritten recursive descent parsers, and hence it may be considered more intuitive to understand. Further, it comes with attractive properties such as linear time parsing. One should be aware that while the grammar looks like a Context Free Grammar, the language described by a PEG may be different (only LL(1) grammars are guaranteed to represent the same language for both PEGs and other parsers), while other behaviors could be surprising~\\cite{redziejowski2008}. We look at the implementation of a simple PEG parser next.\n",
    "\n",
    "For simplicity, we do not implent PEG predicates. However, the resulting parser is robust enough for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We derive from the `Parser` base class first, and we accept the text to be parsed in the `parse` method, which in turn calls `unify_key` with the `start_symbol` which is the starting point of `PEG` parsing.\n",
    "\n",
    "The algorithm itself is simple. It tries to unify the production rules corresponding to the start symbol to the given text. For that, it first verifies that the start symbol is present in the grammar. Next, it retrieves the production rules corresponding to the start symbol, and tries to unify each rule in order using `unify_rule`. If *any* of the rules succeed in being unified with the given text, the parse is considered a success.\n",
    "\n",
    "The `unify_rule` is similar. It retrieves the tokens corresponding to the rule that it needs to unify with the text, and calls `unify_key` on them in sequence. If *all* tokens are successfully unified with the text, the parse is a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser(Parser):\n",
    "    def __init__(self, grammar, start_symbol):\n",
    "        super().__init__(canonical(grammar), start_symbol)\n",
    "        \n",
    "    def parse(self, text):\n",
    "        return self.unify_key(self.start_symbol, text, 0)\n",
    "    \n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if key not in self.grammar:\n",
    "            if text[at:].startswith(key): return at + len(key), (key, [])\n",
    "            else: return at, None\n",
    "        for rule in self.grammar[key]:\n",
    "            l, res = self.unify_rule(rule, text, at)\n",
    "            if res: return (l, (key, res))\n",
    "        return 0, None\n",
    "\n",
    "    def unify_rule(self, rule, text, at):\n",
    "        results = []\n",
    "        for token in rule:\n",
    "            at, res = self.unify_key(token, text, at)\n",
    "            if res is None: return at, None\n",
    "            results.append(res)\n",
    "        return at, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap initialization and calling of PEGParser in a method `parse` that accepts the text to be parsed along with the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text, grammar):\n",
    "    peg = PEGParser(grammar, START_SYMBOL)\n",
    "    return peg.parse(text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the consequences of using parsing expression grammars is that one need to be aware of the restrictions it imposes on the grammar. In particular, the production rules need to be written in such a way that the longest match is always tried first. Considering the definition of our original `EXPR_GRAMMAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_GRAMMAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for the key `<factor>`, the definition contains two rules `<integer>` and `<integer>.<integer>`. Due to the way parsing expression grammars work, this ordering is illegal. Hence, we modify our grammar to have the right ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_EXPR_GRAMMAR = {'<start>': ['<expr>'],\n",
    " '<expr>': ['<term> + <expr>', '<term> - <expr>', '<term>'],\n",
    " '<term>': ['<factor> * <term>', '<factor> / <term>', '<factor>'],\n",
    " '<factor>': ['+<factor>',\n",
    "  '-<factor>',\n",
    "  '(<expr>)',\n",
    "  '<integer>.<integer>',\n",
    "  '<integer>'],\n",
    " '<integer>': ['<digit><integer>', '<digit>'],\n",
    " '<digit>': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor, tree = parse(\"1 + (2 * 3)\", NEW_EXPR_GRAMMAR)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor, tree = parse(\"1 * (2 + 3.35)\", NEW_EXPR_GRAMMAR)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While _Parsing Expression Grammars_ are simple at first sight, their behavior in some cases might be a bit unintuitive. For example, here is an example from Redziejowski~\\cite{redziejowski}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEG_SURPRISE = {\n",
    "    \"A\": [\"a<A>a\",\"aa\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreted as a context free grammar and used as a string generator, it will produce strings of the form `a, aa, aaaa, aaaaaa` that is, it produces strings where the number of `a` is \\Latex{2*n}. However, the PEG can only recognize strings of the form `a, aa, aaaa, aaaaaaaa`, that is where the number of `a` is \\Latex{2^n}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table driven parsers\n",
    "\n",
    "Parsing Expression Grammars specifically oriented towards writing recognizers. Unfortunately, Parsing Expression Grammars are not suitable for grammar based fuzzing. \\todo{Verify, and explain how precedence in parsing is not translatable to generation}.\n",
    "\\todo{Explain LL(k), LR(k), and general Context-Free parsers such as Early and CYK parsers}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LL(1) parser\n",
    "\n",
    "LL(k) parsers are top-down parsers that rely on a lookahead of k tokens. We provide an implementation of an LL(1) parser.\n",
    "\n",
    "We first need to define a few tokens that will come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOF = '\\0'\n",
    "EPSILON = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LL(1) grammars are rather restrictive. Specifically, the grammar should not contain left recursion. Hence, we have to\n",
    "update our original grammar to remove left-recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {'<start>': ['<expr>'],\n",
    "           '<expr>': ['<term><expr_>'],\n",
    "           '<expr_>': ['+<expr>',\n",
    "                       '-<expr>',\n",
    "                       ''],\n",
    "           '<term>': ['<factor><term_>'],\n",
    "           '<term_>': ['*<term>',\n",
    "                       '/<term>',\n",
    "                       ''],\n",
    "           '<factor>': ['+<factor>',\n",
    "                        '-<factor>',\n",
    "                        '(<expr>)',\n",
    "                        '<int>'],\n",
    "           '<int>': ['<integer><integer_>'],\n",
    "           '<integer_>': ['',\n",
    "                          '.<integer>'],\n",
    "           '<integer>': ['<digit><I>'],\n",
    "           '<I>': ['<integer>',\n",
    "                   ''],\n",
    "           '<digit>': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to change the grammar so that the productions become tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [split(e) for e in grammar[k]] for k in grammar}\n",
    "new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get the listing of production rules, and the set of terminals in the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(g): return [(k, e) for k, a in g.items() for e in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminals(g):\n",
    "    return set(t for k, expr in rules(g) for t in expr if t not in g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and Follow sets\n",
    "\n",
    "\\todo{Define first and follow sets}\n",
    "We first define the fixpiont\n",
    "\\todo{Define what is a fixpoint of a function}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixpoint(f):\n",
    "    def helper(*args):\n",
    "        while True:\n",
    "            sargs = repr(args)\n",
    "            args_ = f(*args)\n",
    "            if repr(args_) == sargs:\n",
    "                return args\n",
    "            args = args_\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fixpoint\n",
    "def nullable_(rules, e):\n",
    "    for A, expression in rules:\n",
    "        if all((token in e)  for token in expression): e |= {A}\n",
    "    return (rules, e)\n",
    "\n",
    "def nullable(grammar):\n",
    "    return nullable_(rules(grammar), set())[1]\n",
    "\n",
    "\n",
    "@fixpoint\n",
    "def firstset_(rules, first, epsilon):\n",
    "    for A, expression in rules:\n",
    "        for token in expression:\n",
    "            first[A] |= first[token]\n",
    "\n",
    "            # update until the first token that is not nullable\n",
    "            if token not in epsilon:\n",
    "                break\n",
    "    return (rules, first, epsilon)\n",
    "\n",
    "def firstset(grammar, epsilon):\n",
    "    # https://www.cs.umd.edu/class/spring2014/cmsc430/lectures/lec05.pdf p6\n",
    "    # (1) If X is a terminal, then First(X) is just X\n",
    "    first = {i:{i} for i in terminals(grammar)}\n",
    "\n",
    "    # (2) if X ::= epsilon, then epsilon \\in First(X)\n",
    "    for k in grammar:\n",
    "        first[k] = {EPSILON} if k in epsilon else set()\n",
    "    return firstset_(rules(grammar), first, epsilon)[1]\n",
    "\n",
    "@fixpoint\n",
    "def followset_(grammar, epsilon, first, follow):\n",
    "    for A, expression in rules(grammar):\n",
    "        # https://www.cs.umd.edu/class/spring2014/cmsc430/lectures/lec05.pdf\n",
    "        # https://www.cs.uaf.edu/~cs331/notes/FirstFollow.pdf\n",
    "        # essentially, we start from the end of the expression. Then:\n",
    "        # (3) if there is a production A -> aB, then every thing in\n",
    "        # FOLLOW(A) is in FOLLOW(B)\n",
    "        # note: f_B serves as both follow and first.\n",
    "        f_B = follow[A]\n",
    "        for t in reversed(expression):\n",
    "            # update the follow for the current token. If this is the\n",
    "            # first iteration, then here is the assignment\n",
    "            if t in grammar:\n",
    "                follow[t] |= f_B  # only bother with nt\n",
    "\n",
    "            # computing the last follow symbols for each token t. This\n",
    "            # will be used in the next iteration. If current token is\n",
    "            # nullable, then previous follows can be a legal follow for\n",
    "            # next. Else, only the first of current token is legal follow\n",
    "            # essentially\n",
    "\n",
    "            # (2) if there is a production A -> aBb then everything in FIRST(B)\n",
    "            # except for epsilon is added to FOLLOW(B)\n",
    "            f_B = f_B | first[t] if t in epsilon else (first[t] - {EPSILON})\n",
    "\n",
    "    return (grammar, epsilon, first, follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followset(grammar, start):\n",
    "    # Initialize first and follow sets for non-terminals\n",
    "    follow = {i: set() for i in grammar}\n",
    "    follow[start] = {EOF}\n",
    "\n",
    "    epsilon = nullable(grammar)\n",
    "    first = firstset(grammar, epsilon)\n",
    "    return followset_(grammar, epsilon, first, follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnullable(rule, epsilon):\n",
    "    return all(token in epsilon for token in rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfirst(rule, first, epsilon):\n",
    "    tokens = set()\n",
    "    for token in rule:\n",
    "        tokens |= first[token]\n",
    "        if token not in epsilon: break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(rulepair, first, follow, epsilon):\n",
    "    A, rule = rulepair\n",
    "    rf = rfirst(rule, first, epsilon)\n",
    "    if rnullable(rule, epsilon):\n",
    "        rf |= follow[A]\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table(grammar, start, my_rules):\n",
    "    _, epsilon, first, follow = followset(grammar, start)\n",
    "\n",
    "    ptable = [(rule, predict(rule, first, follow, epsilon))\n",
    "              for rule in my_rules]\n",
    "\n",
    "    parse_tbl = {k: {} for k in grammar}\n",
    "\n",
    "    for (k, expr), pvals in ptable:\n",
    "        parse_tbl[k].update({v: (k, expr) for v in pvals})\n",
    "    return parse_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_helper(grammar, tbl, stack, inplst):\n",
    "    inp, *inplst = inplst\n",
    "    exprs = []\n",
    "    while stack:\n",
    "        val, *stack = stack\n",
    "        if isinstance(val, tuple):\n",
    "            exprs.append(val)\n",
    "        elif val not in grammar:  # terminal\n",
    "            assert val == inp\n",
    "            exprs.append(val)\n",
    "            inp, *inplst = inplst or [None]\n",
    "        else:\n",
    "            _, rhs = tbl[val][inp] if inp else (None, [])\n",
    "            stack = rhs + [(val, len(rhs))] + stack\n",
    "    return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(grammar, start, inp):\n",
    "    my_rules = rules(grammar)\n",
    "    parse_tbl = parse_table(grammar, start, my_rules)\n",
    "    k, _ = my_rules[0]\n",
    "    stack = [k]\n",
    "    return parse_helper(grammar, parse_tbl, stack, list(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_to_tree(arr):\n",
    "    stack = []\n",
    "    while arr:\n",
    "        elt = arr.pop(0)\n",
    "        if not isinstance(elt, tuple):\n",
    "            stack.append((elt, []))\n",
    "        else:\n",
    "            # get the last n\n",
    "            sym, n = elt\n",
    "            elts = stack[-n:] if n > 0 else []\n",
    "            stack = stack[0:len(stack) - n]\n",
    "            stack.append((sym, elts))\n",
    "    assert len(stack) == 1\n",
    "    return stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = linear_to_tree(parse(new_grammar, START_SYMBOL, '(1+2)*3'))\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earley parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(rule): return [i.strip() for i in rule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [shrink(split(e)) for e in EXPR_GRAMMAR[k]] for k in EXPR_GRAMMAR}\n",
    "new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fixpoint\n",
    "def nullable_(rules, e):\n",
    "    for A, expression in rules:\n",
    "        if all((token in e)  for token in expression): e |= {A}\n",
    "    return (rules, e)\n",
    "\n",
    "def nullable(grammar):\n",
    "    return nullable_(rules(grammar), set())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, name, expr, dot, origin, children=[]):\n",
    "        self.name, self.expr, self.dot, self.origin = name, expr, dot, origin\n",
    "        self.children = children[:]\n",
    "    def finished(self): return self.dot >= len(self.expr)\n",
    "    def shift(self):\n",
    "        return State(self.name, self.expr, self.dot+1, self.origin, self.children)\n",
    "    def symbol(self): return self.expr[self.dot]\n",
    "\n",
    "    def _t(self): return (self.name, self.expr, self.dot, self.origin.i, tuple(self.children))\n",
    "    def __hash__(self): return hash(self._t())\n",
    "    def __eq__(self, other): return  self._t() == other._t()\n",
    "\n",
    "class Column(object):\n",
    "    def __init__(self, i, token):\n",
    "        self.token, self.states, self._unique, self.i = token, [], {}, i\n",
    "\n",
    "    def add(self, state):\n",
    "        if state in self._unique: return self._unique[state]\n",
    "        self._unique[state] = state\n",
    "        self.states.append(state)\n",
    "        return self._unique[state]\n",
    "\n",
    "def predict(col, sym, grammar):\n",
    "    for alt in grammar[sym]:\n",
    "        col.add(State(sym, tuple(alt), 0, col))\n",
    "\n",
    "def scan(col, state, token):\n",
    "    if token == col.token:\n",
    "        col.add(state.shift())\n",
    "\n",
    "def complete(col, state, grammar):\n",
    "    for st in state.origin.states:\n",
    "        if st.finished(): continue\n",
    "        if state.name != st.symbol(): continue\n",
    "        col.add(st.shift()).children.append(state)\n",
    "        \n",
    "class EarleyParser(Parser):\n",
    "    def __init__(self, grammar, start_symbol):\n",
    "        super().__init__(canonical(grammar), start_symbol)\n",
    "\n",
    "    # http://courses.washington.edu/ling571/ling571_fall_2010/slides/parsing_earley.pdf\n",
    "    # https://github.com/tomerfiliba/tau/blob/master/earley3.py\n",
    "    def _parse(words, grammar, start):\n",
    "        # Aycock 2002 Practical Earley Parsing -- treatment of epsilon\n",
    "        epsilon = nullable(grammar)\n",
    "        alt = tuple(*grammar[start])\n",
    "        chart = [Column(i, tok) for i,tok in enumerate([None, *words])]\n",
    "        chart[0].add(State(start, alt, 0, chart[0], []))\n",
    "\n",
    "        for i, col in enumerate(chart):\n",
    "            for state in col.states:\n",
    "                if state.finished():\n",
    "                    complete(col, state, grammar)\n",
    "                else:\n",
    "                    sym = state.symbol()\n",
    "                    if sym in grammar:\n",
    "                        predict(col, sym, grammar)\n",
    "                        if sym in epsilon:\n",
    "                            # note that precomputation of epsilon derivation can result in infinite\n",
    "                            # loops for certain grammars. Hence, we mark a nullable non-terminal\n",
    "                            # but do not expand it.\n",
    "                            col.add(state.shift()).children.append(State(sym + '*', tuple(), 0, col))\n",
    "                    else:\n",
    "                        if i + 1 >= len(chart): continue\n",
    "                        scan(chart[i+1], state, sym)\n",
    "        return chart\n",
    "    \n",
    "    def parse(self, text):\n",
    "        table = self._parse(text, )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_expr(expr, children, grammar):\n",
    "    terms = iter([(i,[]) for i in expr if i not in grammar])\n",
    "    nts = iter([node_translator(i, grammar) for i in  children])\n",
    "    return [next(terms if i not in grammar else nts) for i in expr]\n",
    "\n",
    "def node_translator(state, grammar):\n",
    "    return (state.name, process_expr(state.expr, state.children, grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [shrink(split(e)) for e in EXPR_GRAMMAR[k]] for k in EXPR_GRAMMAR}\n",
    "table = parse(list('1+2+3'), new_grammar, '<start>')\n",
    "states = [st for st in table[-1].states if st.name == '<start>' and st.finished()]\n",
    "for state in states:\n",
    "    display_tree(node_translator(state, new_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguous grammars generates parse forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar= {\n",
    "        '<start>': ['<A>'],\n",
    "        '<A>': ['<A>+<A>', 'a'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grammar = {k: [shrink(split(e)) for e in grammar[k]] for k in grammar}\n",
    "table = parse(list('a+a+a'), new_grammar, '<start>')\n",
    "states = [st for st in table[-1].states if st.name == '<start>' and st.finished()]\n",
    "for state in states:\n",
    "    display_tree(node_translator(state, new_grammar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Lesson one_\n",
    "* _Lesson two_\n",
    "* _Lesson three_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "_Link to subsequent chapters (notebooks) here, as in:_\n",
    "\n",
    "* [use _mutations_ on existing inputs to get more valid inputs](MutationFuzzer.ipynb)\n",
    "* [use _grammars_ (i.e., a specification of the input format) to get even more valid inputs](Grammars.ipynb)\n",
    "* [reduce _failing inputs_ for efficient debugging](Reducer.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Close the chapter with a few exercises such that people have things to do.  In Jupyter Notebook, use the `exercise2` nbextension to add solutions that can be interactively viewed or hidden:\n",
    "\n",
    "* Mark the _last_ cell of the exercise (this should be a _text_ cell) as well as _all_ cells of the solution.  (Use the `rubberband` nbextension and use Shift+Drag to mark multiple cells.)\n",
    "* Click on the `solution` button at the top.\n",
    "\n",
    "(Alternatively, just copy the exercise and solution cells below with their metadata.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Some code that is part of the exercise\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "_Some more text for the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "_Some text for the solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Some code for the solution\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "_Some more text for the solution_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "_Text of the exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "_Solution for the exercise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
